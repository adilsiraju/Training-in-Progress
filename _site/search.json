[
  {
    "objectID": "posts/python/modules.html",
    "href": "posts/python/modules.html",
    "title": "Python Modules and Packages",
    "section": "",
    "text": "A module is a file containing Python definitions and statements. The file name is the module name with the suffix .py added.\n\n# Example: Using built-in modules\nimport math\nimport datetime\n\n# Using math module\nprint(f\"Pi value: {math.pi}\")\nprint(f\"Square root of 16: {math.sqrt(16)}\")\n\n# Using datetime module\nnow = datetime.datetime.now()\nprint(f\"Current time: {now}\")\n\nPi value: 3.141592653589793\nSquare root of 16: 4.0\nCurrent time: 2025-09-18 11:56:02.592034"
  },
  {
    "objectID": "posts/python/modules.html#what-are-modules",
    "href": "posts/python/modules.html#what-are-modules",
    "title": "Python Modules and Packages",
    "section": "",
    "text": "A module is a file containing Python definitions and statements. The file name is the module name with the suffix .py added.\n\n# Example: Using built-in modules\nimport math\nimport datetime\n\n# Using math module\nprint(f\"Pi value: {math.pi}\")\nprint(f\"Square root of 16: {math.sqrt(16)}\")\n\n# Using datetime module\nnow = datetime.datetime.now()\nprint(f\"Current time: {now}\")\n\nPi value: 3.141592653589793\nSquare root of 16: 4.0\nCurrent time: 2025-09-18 11:56:02.592034"
  },
  {
    "objectID": "posts/python/modules.html#import-statements",
    "href": "posts/python/modules.html#import-statements",
    "title": "Python Modules and Packages",
    "section": "Import Statements",
    "text": "Import Statements\nThere are several ways to import modules in Python:\n\n# 1. Import entire module\nimport numpy\n\n# 2. Import with alias\nimport numpy as np\n\n# 3. Import specific functions\nfrom math import sqrt, pi\n\n# 4. Import all (not recommended)\n# from math import *\n\n# Examples\narr = np.array([1, 2, 3, 4, 5])\nprint(f\"Array: {arr}\")\nprint(f\"Mean: {np.mean(arr)}\")\n\nprint(f\"Using imported sqrt: {sqrt(25)}\")\nprint(f\"Using imported pi: {pi}\")\n\nArray: [1 2 3 4 5]\nMean: 3.0\nUsing imported sqrt: 5.0\nUsing imported pi: 3.141592653589793"
  },
  {
    "objectID": "posts/python/modules.html#creating-custom-modules",
    "href": "posts/python/modules.html#creating-custom-modules",
    "title": "Python Modules and Packages",
    "section": "Creating Custom Modules",
    "text": "Creating Custom Modules\nYou can create your own modules by saving Python code in a .py file.\n\n# Example: Creating a simple utility module (conceptually)\n# This would be saved as utils.py\n\ndef greet(name):\n    \"\"\"Greet a person by name\"\"\"\n    return f\"Hello, {name}!\"\n\ndef calculate_area(radius):\n    \"\"\"Calculate area of a circle\"\"\"\n    import math\n    return math.pi * radius ** 2\n\n# Constants\nVERSION = \"1.0.0\"\nAUTHOR = \"Mohammed Adil Siraju\"\n\n# Test the functions\nprint(greet(\"Adil\"))\nprint(f\"Area of circle with radius 5: {calculate_area(5):.2f}\")\nprint(f\"Module version: {VERSION}\")\n\nHello, Adil!\nArea of circle with radius 5: 78.54\nModule version: 1.0.0"
  },
  {
    "objectID": "posts/python/modules.html#packages",
    "href": "posts/python/modules.html#packages",
    "title": "Python Modules and Packages",
    "section": "Packages",
    "text": "Packages\nA package is a collection of modules organized in directories. Packages help organize related modules together.\n\n# Example package structure:\n# mypackage/\n#     __init__.py\n#     module1.py\n#     module2.py\n#     subpackage/\n#         __init__.py\n#         submodule.py\n\n# Importing from packages\n# import mypackage.module1\n# from mypackage import module2\n# from mypackage.subpackage import submodule\n\nprint(\"Package structure example shown above\")\n\nPackage structure example shown above"
  },
  {
    "objectID": "posts/python/modules.html#popular-python-packages-for-data-science",
    "href": "posts/python/modules.html#popular-python-packages-for-data-science",
    "title": "Python Modules and Packages",
    "section": "Popular Python Packages for Data Science",
    "text": "Popular Python Packages for Data Science\nLet’s explore some essential packages for AI/ML work:\n\n# Data manipulation and analysis\nimport pandas as pd\nimport numpy as np\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Create sample data\ndata = {\n    'x': [1, 2, 3, 4, 5],\n    'y': [2, 4, 6, 8, 10]\n}\n\ndf = pd.DataFrame(data)\nprint(\"Sample DataFrame:\")\nprint(df)\n\n# Simple linear regression example\nX = df[['x']]\ny = df['y']\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\npredictions = model.predict(X)\nmse = mean_squared_error(y, predictions)\n\nprint(f\"\\nLinear Regression Results:\")\nprint(f\"Slope: {model.coef_[0]:.2f}\")\nprint(f\"Intercept: {model.intercept_:.2f}\")\nprint(f\"MSE: {mse:.2f}\")\n\nSample DataFrame:\n   x   y\n0  1   2\n1  2   4\n2  3   6\n3  4   8\n4  5  10\n\nLinear Regression Results:\nSlope: 2.00\nIntercept: -0.00\nMSE: 0.00"
  },
  {
    "objectID": "posts/python/modules.html#best-practices",
    "href": "posts/python/modules.html#best-practices",
    "title": "Python Modules and Packages",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse descriptive names for modules and packages\nKeep modules focused on a single responsibility\nUse __init__.py to control package imports\nDocument your modules with docstrings\nFollow PEP 8 naming conventions\nAvoid circular imports"
  },
  {
    "objectID": "posts/python/modules.html#key-takeaways",
    "href": "posts/python/modules.html#key-takeaways",
    "title": "Python Modules and Packages",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nModules help organize and reuse code\nPackages group related modules together\nImport statements control what code is available\nPython’s standard library provides many useful modules\nThird-party packages extend Python’s capabilities\n\n\nNext: We’ll explore Python functions and classes in detail."
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html",
    "href": "posts/pandas/Working with multi index df.html",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "This notebook covers MultiIndex DataFrames in pandas: creation, inspection, and combining with NumPy arrays. MultiIndex allows hierarchical indexing for complex data structures.\n\n\nMultiIndex in pandas allows you to have multiple levels of indexing on rows or columns. It’s useful for hierarchical data like time series with multiple categories.\n\nimport pandas as pd\n\n\n\n\nUse pd.MultiIndex.from_arrays() to create a MultiIndex from arrays. Here, we create a DataFrame with a two-level row index.\n\narrays = [['A','A','B','B',], [1,2,1,2]]\nindex = pd.MultiIndex.from_arrays(arrays, names=('First', 'Second'))\ndf = pd.DataFrame({'Data': [10,20,30,40]}, index=index)\n\ndf\n\n\n\n\n\n\n\n\n\nData\n\n\nFirst\nSecond\n\n\n\n\n\nA\n1\n10\n\n\n2\n20\n\n\nB\n1\n30\n\n\n2\n40\n\n\n\n\n\n\n\n\n\n\nAccess the index with df.index. It shows the hierarchical structure.\n\ndf.index\n\nMultiIndex([('A', 1),\n            ('A', 2),\n            ('B', 1),\n            ('B', 2)],\n           names=['First', 'Second'])\n\n\n\n\n\nPandas integrates seamlessly with NumPy. You can create DataFrames from NumPy arrays and use NumPy functions on DataFrame data.\n\n\n\nUse pd.DataFrame() with a NumPy array to create a DataFrame. Specify column names for clarity.\n\nimport numpy as np\n\n\narray_data = np.array([[1,2,3], [4,5,6], [7,8,9]])\ndf_np = pd.DataFrame(array_data, columns=['Sales A', 'Sales B', 'Sales C'])\n\ndf_np\n\n\n\n\n\n\n\n\nSales A\nSales B\nSales C\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9\n\n\n\n\n\n\n\n\n\n\n\nUse meaningful names for MultiIndex levels (e.g., ‘Category’, ‘Subcategory’).\nWhen selecting data, use .loc[] with tuples for MultiIndex access.\nReset index with df.reset_index() if you need to flatten the hierarchy.\n\n\n\n\nThis notebook demonstrated creating and inspecting MultiIndex DataFrames and integrating pandas with NumPy arrays. MultiIndex is powerful for complex data but can be tricky—practice with real datasets!"
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#introduction-to-multiindex",
    "href": "posts/pandas/Working with multi index df.html#introduction-to-multiindex",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "MultiIndex in pandas allows you to have multiple levels of indexing on rows or columns. It’s useful for hierarchical data like time series with multiple categories.\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#creating-a-multiindex-dataframe",
    "href": "posts/pandas/Working with multi index df.html#creating-a-multiindex-dataframe",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "Use pd.MultiIndex.from_arrays() to create a MultiIndex from arrays. Here, we create a DataFrame with a two-level row index.\n\narrays = [['A','A','B','B',], [1,2,1,2]]\nindex = pd.MultiIndex.from_arrays(arrays, names=('First', 'Second'))\ndf = pd.DataFrame({'Data': [10,20,30,40]}, index=index)\n\ndf\n\n\n\n\n\n\n\n\n\nData\n\n\nFirst\nSecond\n\n\n\n\n\nA\n1\n10\n\n\n2\n20\n\n\nB\n1\n30\n\n\n2\n40"
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#inspecting-the-multiindex",
    "href": "posts/pandas/Working with multi index df.html#inspecting-the-multiindex",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "Access the index with df.index. It shows the hierarchical structure.\n\ndf.index\n\nMultiIndex([('A', 1),\n            ('A', 2),\n            ('B', 1),\n            ('B', 2)],\n           names=['First', 'Second'])"
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#combining-pandas-with-numpy",
    "href": "posts/pandas/Working with multi index df.html#combining-pandas-with-numpy",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "Pandas integrates seamlessly with NumPy. You can create DataFrames from NumPy arrays and use NumPy functions on DataFrame data."
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#creating-dataframes-from-numpy-arrays",
    "href": "posts/pandas/Working with multi index df.html#creating-dataframes-from-numpy-arrays",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "Use pd.DataFrame() with a NumPy array to create a DataFrame. Specify column names for clarity.\n\nimport numpy as np\n\n\narray_data = np.array([[1,2,3], [4,5,6], [7,8,9]])\ndf_np = pd.DataFrame(array_data, columns=['Sales A', 'Sales B', 'Sales C'])\n\ndf_np\n\n\n\n\n\n\n\n\nSales A\nSales B\nSales C\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9"
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#best-practices",
    "href": "posts/pandas/Working with multi index df.html#best-practices",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "Use meaningful names for MultiIndex levels (e.g., ‘Category’, ‘Subcategory’).\nWhen selecting data, use .loc[] with tuples for MultiIndex access.\nReset index with df.reset_index() if you need to flatten the hierarchy."
  },
  {
    "objectID": "posts/pandas/Working with multi index df.html#summary",
    "href": "posts/pandas/Working with multi index df.html#summary",
    "title": "Working with MultiIndex DataFrames",
    "section": "",
    "text": "This notebook demonstrated creating and inspecting MultiIndex DataFrames and integrating pandas with NumPy arrays. MultiIndex is powerful for complex data but can be tricky—practice with real datasets!"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html",
    "href": "posts/pandas/text manipulation methods.html",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "This notebook explores pandas string methods (accessed via .str) for manipulating text data in DataFrames. Covers case changes, searching, regex, replacement, and splitting.\n\n\nPandas provides vectorized string operations through the .str accessor. These methods work on Series of strings and are efficient for text data processing.\n\nimport pandas as pd\n\n\n\n\nWe’ll use a simple DataFrame with text data to demonstrate string methods.\n\ndata = {\n    'TextData': ['Hello','World','Python', 'Pandas', 'Data Science']\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nTextData\n\n\n\n\n0\nHello\n\n\n1\nWorld\n\n\n2\nPython\n\n\n3\nPandas\n\n\n4\nData Science\n\n\n\n\n\n\n\n\n\n\nConvert text to lowercase or uppercase using .str.lower() and .str.upper().\n\ndf['LowerCase'] = df['TextData'].str.lower()\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\n\n\n\n\n0\nHello\nhello\n\n\n1\nWorld\nworld\n\n\n2\nPython\npython\n\n\n3\nPandas\npandas\n\n\n4\nData Science\ndata science\n\n\n\n\n\n\n\n\ndf['UpperCase'] = df['TextData'].str.upper()\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\n\n\n\n\n0\nHello\nhello\nHELLO\n\n\n1\nWorld\nworld\nWORLD\n\n\n2\nPython\npython\nPYTHON\n\n\n3\nPandas\npandas\nPANDAS\n\n\n4\nData Science\ndata science\nDATA SCIENCE\n\n\n\n\n\n\n\n\n\n\nCheck if strings contain substrings with .str.contains(). Use case=False for case-insensitive search.\n\ndf['Contains'] = df['TextData'].str.contains('O', case=False)\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n\n\n1\nWorld\nworld\nWORLD\nTrue\n\n\n2\nPython\npython\nPYTHON\nTrue\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n\n\n\n\n\n\n\n\n\n\nUse regex with methods like .str.findall() to find patterns. Here, finding all ‘o’ characters.\n\ndf['Matches'] = df['TextData'].str.findall('o')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]\n\n\n\n\n\n\n\n\n\n\nReplace substrings with .str.replace() and split strings with .str.split().\n\ndf['Replaced'] = df['TextData'].str.replace('o', 'x')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\nReplaced\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\nHellx\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\nWxrld\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\nPythxn\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\nPandas\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]\nData Science\n\n\n\n\n\n\n\n\ndf['Split'] = df['TextData'].str.split(' ')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\nReplaced\nSplit\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\nHellx\n[Hello]\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\nWxrld\n[World]\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\nPythxn\n[Python]\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\nPandas\n[Pandas]\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]\nData Science\n[Data, Science]\n\n\n\n\n\n\n\n\n\n\n\nHandle missing values: Use .str methods which handle NaN gracefully.\nFor complex regex, test patterns separately.\nVectorized operations are faster than loops.\n\n\n\n\nThis notebook covered essential pandas string methods for text manipulation. Experiment with real datasets to master these techniques!"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#introduction",
    "href": "posts/pandas/text manipulation methods.html#introduction",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Pandas provides vectorized string operations through the .str accessor. These methods work on Series of strings and are efficient for text data processing.\n\nimport pandas as pd"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#sample-data",
    "href": "posts/pandas/text manipulation methods.html#sample-data",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "We’ll use a simple DataFrame with text data to demonstrate string methods.\n\ndata = {\n    'TextData': ['Hello','World','Python', 'Pandas', 'Data Science']\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nTextData\n\n\n\n\n0\nHello\n\n\n1\nWorld\n\n\n2\nPython\n\n\n3\nPandas\n\n\n4\nData Science"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#case-conversion",
    "href": "posts/pandas/text manipulation methods.html#case-conversion",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Convert text to lowercase or uppercase using .str.lower() and .str.upper().\n\ndf['LowerCase'] = df['TextData'].str.lower()\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\n\n\n\n\n0\nHello\nhello\n\n\n1\nWorld\nworld\n\n\n2\nPython\npython\n\n\n3\nPandas\npandas\n\n\n4\nData Science\ndata science\n\n\n\n\n\n\n\n\ndf['UpperCase'] = df['TextData'].str.upper()\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\n\n\n\n\n0\nHello\nhello\nHELLO\n\n\n1\nWorld\nworld\nWORLD\n\n\n2\nPython\npython\nPYTHON\n\n\n3\nPandas\npandas\nPANDAS\n\n\n4\nData Science\ndata science\nDATA SCIENCE"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#searching-in-text",
    "href": "posts/pandas/text manipulation methods.html#searching-in-text",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Check if strings contain substrings with .str.contains(). Use case=False for case-insensitive search.\n\ndf['Contains'] = df['TextData'].str.contains('O', case=False)\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n\n\n1\nWorld\nworld\nWORLD\nTrue\n\n\n2\nPython\npython\nPYTHON\nTrue\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#regular-expressions-regex",
    "href": "posts/pandas/text manipulation methods.html#regular-expressions-regex",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Use regex with methods like .str.findall() to find patterns. Here, finding all ‘o’ characters.\n\ndf['Matches'] = df['TextData'].str.findall('o')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#replacement-and-splitting",
    "href": "posts/pandas/text manipulation methods.html#replacement-and-splitting",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Replace substrings with .str.replace() and split strings with .str.split().\n\ndf['Replaced'] = df['TextData'].str.replace('o', 'x')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\nReplaced\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\nHellx\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\nWxrld\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\nPythxn\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\nPandas\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]\nData Science\n\n\n\n\n\n\n\n\ndf['Split'] = df['TextData'].str.split(' ')\ndf\n\n\n\n\n\n\n\n\nTextData\nLowerCase\nUpperCase\nContains\nMatches\nReplaced\nSplit\n\n\n\n\n0\nHello\nhello\nHELLO\nTrue\n[o]\nHellx\n[Hello]\n\n\n1\nWorld\nworld\nWORLD\nTrue\n[o]\nWxrld\n[World]\n\n\n2\nPython\npython\nPYTHON\nTrue\n[o]\nPythxn\n[Python]\n\n\n3\nPandas\npandas\nPANDAS\nFalse\n[]\nPandas\n[Pandas]\n\n\n4\nData Science\ndata science\nDATA SCIENCE\nFalse\n[]\nData Science\n[Data, Science]"
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#best-practices",
    "href": "posts/pandas/text manipulation methods.html#best-practices",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "Handle missing values: Use .str methods which handle NaN gracefully.\nFor complex regex, test patterns separately.\nVectorized operations are faster than loops."
  },
  {
    "objectID": "posts/pandas/text manipulation methods.html#summary",
    "href": "posts/pandas/text manipulation methods.html#summary",
    "title": "Text Manipulation Methods in pandas",
    "section": "",
    "text": "This notebook covered essential pandas string methods for text manipulation. Experiment with real datasets to master these techniques!"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "",
    "text": "Conditional Filtering: Using boolean indexing to filter data based on conditions\nQuery Method: Writing SQL-like queries on pandas DataFrames\n\nSorting Techniques: Multi-column sorting and custom sort orders\nRanking Methods: Different ranking strategies for data analysis\nBest Practices: Efficient data manipulation patterns"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#what-i-learnt",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#what-i-learnt",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "",
    "text": "Conditional Filtering: Using boolean indexing to filter data based on conditions\nQuery Method: Writing SQL-like queries on pandas DataFrames\n\nSorting Techniques: Multi-column sorting and custom sort orders\nRanking Methods: Different ranking strategies for data analysis\nBest Practices: Efficient data manipulation patterns"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#prerequisites",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#prerequisites",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🎯 Prerequisites",
    "text": "🎯 Prerequisites\n\nBasic pandas knowledge\nUnderstanding of Python data structures\nFamiliarity with DataFrame operations\n\nLet’s dive into the advanced filtering techniques!"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#setup-and-data-loading",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#setup-and-data-loading",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "📊 Setup and Data Loading",
    "text": "📊 Setup and Data Loading\nFirst, let’s import pandas and load our sample dataset to work with.\n\nimport pandas as pd\n\n\ndf = pd.read_csv('example.csv')\ndf\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#conditional-filtering-with-boolean-indexing",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#conditional-filtering-with-boolean-indexing",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🔍 Conditional Filtering with Boolean Indexing",
    "text": "🔍 Conditional Filtering with Boolean Indexing\nBoolean indexing allows you to filter DataFrames based on conditions. This is one of the most powerful features of pandas for data manipulation.\n\nBasic Boolean Filtering\nLet’s start with simple conditional filtering using comparison operators.\n\n\nSimple Condition: Filter by Age\nFilter rows where Age is less than 20:\n\ndf[df['Age'] &lt; 20]\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\n\nOR Condition: Multiple Criteria (Either condition true)\nFilter rows where City is ‘Vellore’ OR Age is less than 20:\n\ndf[(df['City']=='Vellore') | (df['Age']&lt;20)]\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\n\nAND Condition: Multiple Criteria (All conditions true)\nFilter rows where City is ‘Vellore’ AND Age is less than 20:\n\ndf[(df['City']=='Vellore') & (df['Age']&lt;20)]\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nAman\n19\nVellore"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#query-method-sql-like-filtering",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#query-method-sql-like-filtering",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🔎 Query Method: SQL-like Filtering",
    "text": "🔎 Query Method: SQL-like Filtering\nThe .query() method provides a more readable way to filter DataFrames using SQL-like syntax. It’s often cleaner than boolean indexing for complex conditions.\n\nBasic Query Syntax\nQuery rows where Age is greater than 10:\n\ndf.query('Age &gt; 10')\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n\n\n\n\n\n\n\nQuery with AND Condition\nQuery rows where Age &gt; 10 AND City equals ‘Matannur’:\n\ndf.query(\"Age &gt; 10 and City == 'Matannur'\")\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n\n\n\n\n\n\n\nQuery with OR Condition\nQuery rows where Age &lt; 18 OR City equals ‘Matannur’:\n\ndf.query(\"Age &lt; 18 or City == 'Matannur'\")\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\n\nQuery with Mathematical Operations\nQuery with mathematical expressions - rows where Age * 2 &gt; 30:\n\ndf.query(\"Age * 2 &gt; 30\")\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#sorting-data",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#sorting-data",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🔄 Sorting Data",
    "text": "🔄 Sorting Data\nSorting is essential for data analysis. Pandas provides flexible sorting capabilities for both single and multiple columns.\n\nBasic Sorting by Single Column\nSort by Age in descending order:\n\ndf.sort_values('Age',ascending=False)\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\n\nMulti-Column Sorting\nSort by Name first (ascending), then by Age (ascending) for ties:\n\ndf.sort_values(['Name', 'Age'],ascending=True)\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n3\nZahra\n9\nKnr\n\n\n2\nZiya\n15\nTly\n\n\n\n\n\n\n\n\ndf\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#ranking-data",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#ranking-data",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🏆 Ranking Data",
    "text": "🏆 Ranking Data\nRanking assigns ordinal numbers to data points. This is useful for competitions, percentiles, and comparative analysis.\n\nDefault Ranking (Average method)\nRank ages - ties get average rank:\n\ndf['Age'].rank()\n\n0    4.0\n1    3.0\n2    2.0\n3    1.0\nName: Age, dtype: float64\n\n\n\n\nRanking with ‘min’ Method\nRank ages - ties get minimum rank:\n\ndf['Age'].rank(method='min')\n\n0    4.0\n1    3.0\n2    2.0\n3    1.0\nName: Age, dtype: float64"
  },
  {
    "objectID": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#summary",
    "href": "posts/pandas/Pandas Advance Techniques of Conditional Filtering and Queriying.html#summary",
    "title": "Pandas: Advanced Techniques for Conditional Filtering and Querying",
    "section": "🎉 Summary",
    "text": "🎉 Summary\nYou’ve now learned advanced pandas techniques for:\n\nConditional Filtering: Using boolean indexing with & (AND) and | (OR) operators\nQuery Method: Writing SQL-like queries with .query() for cleaner, more readable code\nSorting: Single and multi-column sorting with sort_values()\nRanking: Different ranking methods for comparative analysis\n\n\nKey Takeaways\n\nBoolean Indexing: df[condition] is powerful but can get complex with multiple conditions\nQuery Method: df.query('condition') is more readable for complex filtering\nSorting: Use sort_values() with lists for multi-column sorting\nRanking: Choose ranking method based on your analysis needs (average, min, max, etc.)\n\n\n\nNext Steps\n\nPractice with real datasets\nCombine filtering with other pandas operations\nExplore advanced query features with variables\nLearn about method chaining for efficient data pipelines\n\nHappy pandas coding! 🐼"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html",
    "href": "posts/pandas/learn pandas Series.html",
    "title": "Pandas: Intro to Series",
    "section": "",
    "text": "Series is pandas’ one-dimensional labeled array. This notebook covers creating, accessing, and operating on Series.\nYou will learn how to: - Create a Pandas Series - Access elements and slices - Perform arithmetic operations - View Series properties and methods - Sort and describe Series data"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#importing-libraries",
    "href": "posts/pandas/learn pandas Series.html#importing-libraries",
    "title": "Pandas: Intro to Series",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nImport pandas and visualization libraries.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#creating-a-series",
    "href": "posts/pandas/learn pandas Series.html#creating-a-series",
    "title": "Pandas: Intro to Series",
    "section": "Creating a Series",
    "text": "Creating a Series\nCreate a Series from a list with custom index labels.\n\nmy_series = pd.Series([10,20,30,40,50], index=['A', 'B', 'C', 'D', 'E'])\nmy_series\n\nA    10\nB    20\nC    30\nD    40\nE    50\ndtype: int64\n\n\n\ntype(my_series)\n\npandas.core.series.Series"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#accessing-elements",
    "href": "posts/pandas/learn pandas Series.html#accessing-elements",
    "title": "Pandas: Intro to Series",
    "section": "Accessing Elements",
    "text": "Accessing Elements\nAccess elements by label or slice ranges.\n\nmy_series['C':'E']\n\nC    30\nD    40\nE    50\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#arithmetic-operations",
    "href": "posts/pandas/learn pandas Series.html#arithmetic-operations",
    "title": "Pandas: Intro to Series",
    "section": "Arithmetic Operations",
    "text": "Arithmetic Operations\nPerform element-wise operations on Series.\n\nmy_series + 15\n\nA    25\nB    35\nC    45\nD    55\nE    65\ndtype: int64\n\n\n\nmy_series * 25\n\nA     250\nB     500\nC     750\nD    1000\nE    1250\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#series-properties",
    "href": "posts/pandas/learn pandas Series.html#series-properties",
    "title": "Pandas: Intro to Series",
    "section": "Series Properties",
    "text": "Series Properties\nCheck data type, size, and shape of the Series.\n\nmy_series.dtype\n\ndtype('int64')\n\n\n\nmy_series.size\n\n5\n\n\n\nmy_series.shape\n\n(5,)"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#series-methods",
    "href": "posts/pandas/learn pandas Series.html#series-methods",
    "title": "Pandas: Intro to Series",
    "section": "Series Methods",
    "text": "Series Methods\nUse methods like head(), tail(), describe(), and sort_values() for data exploration.\n\nmy_series.head(3)\n\nA    10\nB    20\nC    30\ndtype: int64\n\n\n\nmy_series.tail(2)\n\nD    40\nE    50\ndtype: int64\n\n\n\nmy_series.describe()\n\ncount     5.000000\nmean     30.000000\nstd      15.811388\nmin      10.000000\n25%      20.000000\n50%      30.000000\n75%      40.000000\nmax      50.000000\ndtype: float64\n\n\n\nmy_series.sort_values(ascending=False)\n\nE    50\nD    40\nC    30\nB    20\nA    10\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#best-practices",
    "href": "posts/pandas/learn pandas Series.html#best-practices",
    "title": "Pandas: Intro to Series",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse meaningful index labels for clarity.\nSeries operations are vectorized for performance.\nCheck for NaN values with isna()."
  },
  {
    "objectID": "posts/pandas/learn pandas Series.html#summary",
    "href": "posts/pandas/learn pandas Series.html#summary",
    "title": "Pandas: Intro to Series",
    "section": "Summary",
    "text": "Summary\nThis notebook introduced pandas Series: creation, access, operations, properties, and methods. Series are building blocks for DataFrames!"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html",
    "href": "posts/pandas/learn pandas df operations.html",
    "title": "Pandas: DataFrame Operation",
    "section": "",
    "text": "This notebook covers essential operations on pandas DataFrames: reading, viewing, selecting, filtering, and indexing."
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#reading-data",
    "href": "posts/pandas/learn pandas df operations.html#reading-data",
    "title": "Pandas: DataFrame Operation",
    "section": "Reading Data",
    "text": "Reading Data\nLoad data from files like CSV into DataFrames using pd.read_csv().\n\nimport pandas as pd\n\ndf = pd.read_csv('example.csv')\ndf\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#viewing-data",
    "href": "posts/pandas/learn pandas df operations.html#viewing-data",
    "title": "Pandas: DataFrame Operation",
    "section": "Viewing Data",
    "text": "Viewing Data\nUse head() and tail() to preview the first/last rows of your DataFrame.\n\ndf.head(2)\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n\n\n\n\n\n\ndf.tail(2)\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#selecting-columns",
    "href": "posts/pandas/learn pandas df operations.html#selecting-columns",
    "title": "Pandas: DataFrame Operation",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nSelect specific columns using bracket notation or .loc.\n\ndf[['Name', 'City']]\n\n\n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAdil\nMatannur\n\n\n1\nAman\nVellore\n\n\n2\nZiya\nTly\n\n\n3\nZahra\nKnr\n\n\n\n\n\n\n\n\ndf[['Name', 'City']].values\n\narray([['Adil', 'Matannur'],\n       ['Aman', 'Vellore'],\n       ['Ziya', 'Tly'],\n       ['Zahra', 'Knr']], dtype=object)"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#filtering-data",
    "href": "posts/pandas/learn pandas df operations.html#filtering-data",
    "title": "Pandas: DataFrame Operation",
    "section": "Filtering Data",
    "text": "Filtering Data\nFilter rows based on conditions using boolean indexing.\n\nagegt20 = df[df['Age']&gt;=20]\nagegt20\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n\n\n\n\n\n\nagelt20 = df[df['Age']&lt;20]\nagelt20\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\ndf_ziya = df[df['Name']=='Ziya']\ndf_ziya\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n2\nZiya\n15\nTly\n\n\n\n\n\n\n\n\ndf_mult_cond = df[(df['City']=='Matannur') | (df['Age']&lt;=15 )]\ndf_mult_cond\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\nselected_cities = ['Tly', 'Knr']\ndf_tlyorknr = df[df['City'].isin(selected_cities)]\n\ndf_tlyorknr\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\ndf[df['Name'].str.startswith('Z')]\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#indexing-and-slicing",
    "href": "posts/pandas/learn pandas df operations.html#indexing-and-slicing",
    "title": "Pandas: DataFrame Operation",
    "section": "Indexing and Slicing",
    "text": "Indexing and Slicing\nUse .iloc for position-based and .loc for label-based indexing.\n\ndf.iloc[0:2]\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n\n\n\n\n\n\ndf.loc[df['Age']&gt;20, ['Name', 'City']]\n\n\n\n\n\n\n\n\nName\nCity\n\n\n\n\n0\nAdil\nMatannur"
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#best-practices",
    "href": "posts/pandas/learn pandas df operations.html#best-practices",
    "title": "Pandas: DataFrame Operation",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse .copy() when assigning filtered DataFrames to avoid modifying originals.\nCombine conditions with & and | for complex filters.\nPrefer .loc for clarity in production code."
  },
  {
    "objectID": "posts/pandas/learn pandas df operations.html#summary",
    "href": "posts/pandas/learn pandas df operations.html#summary",
    "title": "Pandas: DataFrame Operation",
    "section": "Summary",
    "text": "Summary\nThis notebook covered key DataFrame operations: reading, viewing, selecting, filtering, and indexing. These form the foundation for data manipulation!"
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html",
    "href": "posts/pandas/Handling Temporal Datas.html",
    "title": "Handling Temporal Data with Pandas",
    "section": "",
    "text": "This notebook covers essential techniques for working with temporal (date and time) data in Pandas. Time-based data is ubiquitous in data science, from financial analysis to IoT sensor data. You’ll learn how to:\nTemporal data handling is crucial for time series analysis, forecasting, and any analysis involving time dimensions."
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html#working-with-dates-and-datetime-objects",
    "href": "posts/pandas/Handling Temporal Datas.html#working-with-dates-and-datetime-objects",
    "title": "Handling Temporal Data with Pandas",
    "section": "1. Working with Dates and Datetime Objects",
    "text": "1. Working with Dates and Datetime Objects\nPandas provides powerful tools for handling date and time data. Let’s start with the fundamentals of datetime conversion and manipulation.\n\nCreating Sample Data with Date Strings\nLet’s start by creating a DataFrame with date information stored as strings. This is a common scenario when loading data from CSV files or databases.\n\nimport pandas as pd\n\ndata = {\n    'Date': ['2023-01-01','2023-03-02','2023-05-03'],\n    'Sales': [100,150,200]\n}\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nDate\nSales\n\n\n\n\n0\n2023-01-01\n100\n\n\n1\n2023-03-02\n150\n\n\n2\n2023-05-03\n200\n\n\n\n\n\n\n\n\n\nChecking Data Types\nNotice that the ‘Date’ column is currently stored as object (string) type. We need to convert it to datetime for proper date operations.\n\ndf.dtypes\n\nDate     object\nSales     int64\ndtype: object\n\n\n\n\nConverting Strings to Datetime\nUse pd.to_datetime() to convert date strings to proper datetime objects. This enables powerful date operations and calculations.\n\ndf['Date'] = pd.to_datetime(df['Date'])\ndf\n\n\n\n\n\n\n\n\nDate\nSales\n\n\n\n\n0\n2023-01-01\n100\n\n\n1\n2023-03-02\n150\n\n\n2\n2023-05-03\n200\n\n\n\n\n\n\n\n\n\nVerifying Datetime Conversion\nNow the ‘Date’ column shows as ‘datetime64[ns]’ type, confirming successful conversion.\n\ndf.dtypes\n\nDate     datetime64[ns]\nSales             int64\ndtype: object\n\n\n\n\nExtracting Date Components\nOnce you have datetime objects, you can easily extract year, month, day, and other components using the .dt accessor. This is useful for grouping and analysis.\n\ndf['Year'] = df['Date'].dt.year\ndf['Month'] = df['Date'].dt.month_name()\ndf['Day'] = df['Date'].dt.day\n\ndf\n\n\n\n\n\n\n\n\nDate\nSales\nYear\nMonth\nDay\n\n\n\n\n0\n2023-01-01\n100\n2023\nJanuary\n1\n\n\n1\n2023-03-02\n150\n2023\nMarch\n2\n\n\n2\n2023-05-03\n200\n2023\nMay\n3"
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html#working-with-time-series-data",
    "href": "posts/pandas/Handling Temporal Datas.html#working-with-time-series-data",
    "title": "Handling Temporal Data with Pandas",
    "section": "2. Working with Time Series Data",
    "text": "2. Working with Time Series Data\nTime series data has dates/times as the index. Pandas provides powerful tools for time series analysis, including resampling, shifting, and date range generation.\n\nCreating Time Series Data\nLet’s create a time series by setting dates as the index and associating values with specific time points.\n\n\nCreating a DateTime Index\nUse pd.date_range() to create sequences of dates. The freq='D' parameter creates daily intervals.\n\ntime_index = pd.date_range('2025-01-01', periods=5, freq='D')\nts_data = pd.Series([100,120,80,110,90], index=time_index)\n\n\n\nViewing Time Series Data\nThe time series now has datetime values as the index, making it easy to perform time-based operations.\n\nts_data\n\n2025-01-01    100\n2025-01-02    120\n2025-01-03     80\n2025-01-04    110\n2025-01-05     90\nFreq: D, dtype: int64\n\n\n\n\nResampling Time Series Data\nResampling changes the frequency of your time series data. Here we resample daily data to weekly frequency using the mean aggregation.\n\nts_resampled = ts_data.resample('W').mean()\nts_resampled\n\n2025-01-05    100.0\nFreq: W-SUN, dtype: float64"
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html#time-series-operations-shifting-and-lagging",
    "href": "posts/pandas/Handling Temporal Datas.html#time-series-operations-shifting-and-lagging",
    "title": "Handling Temporal Data with Pandas",
    "section": "3. Time Series Operations: Shifting and Lagging",
    "text": "3. Time Series Operations: Shifting and Lagging\nShifting operations are crucial for time series analysis, allowing you to compare values across different time periods.\n\nSetting Up Time Series Data for Shifting\nLet’s create a time series dataset and set the date column as the index.\n\ndata = {\n    'Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05'],\n    'Sales': [100,150,200,120,180]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nDate\nSales\n\n\n\n\n0\n2023-01-01\n100\n\n\n1\n2023-01-02\n150\n\n\n2\n2023-01-03\n200\n\n\n3\n2023-01-04\n120\n\n\n4\n2023-01-05\n180\n\n\n\n\n\n\n\n\n\nConverting to Datetime and Setting Index\nConvert the date strings to datetime objects and set the Date column as the DataFrame index.\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\n\n\nSetting Date as Index\nNow the Date column becomes the DataFrame index, enabling time-based operations.\n\ndf.set_index('Date', inplace=True)\ndf\n\n\n\n\n\n\n\n\nSales\n\n\nDate\n\n\n\n\n\n2023-01-01\n100\n\n\n2023-01-02\n150\n\n\n2023-01-03\n200\n\n\n2023-01-04\n120\n\n\n2023-01-05\n180\n\n\n\n\n\n\n\n\n\nShifting Data for Time Series Analysis\nThe shift() method moves data points forward or backward in time. This is essential for calculating period-over-period changes, creating lag features, and time series forecasting.\n\nshift(1): Moves values forward by 1 period (creates lag)\nshift(-1): Moves values backward by 1 period (creates lead)\n\n\ndf['Shifted Sales'] = df['Sales'].shift(1)\ndf['Lagged Sales'] = df['Sales'].shift(-1)\n\n\ndf\n\n\n\n\n\n\n\n\nSales\nShifted Sales\nLagged Sales\n\n\nDate\n\n\n\n\n\n\n\n2023-01-01\n100\nNaN\n150.0\n\n\n2023-01-02\n150\n100.0\n200.0\n\n\n2023-01-03\n200\n150.0\n120.0\n\n\n2023-01-04\n120\n200.0\n180.0\n\n\n2023-01-05\n180\n120.0\nNaN"
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html#generating-date-ranges",
    "href": "posts/pandas/Handling Temporal Datas.html#generating-date-ranges",
    "title": "Handling Temporal Data with Pandas",
    "section": "4. Generating Date Ranges",
    "text": "4. Generating Date Ranges\nPandas makes it easy to create sequences of dates for time series analysis, filling missing dates, or creating time-based indices.\n\nCreating Weekly Date Ranges\nGenerate a sequence of dates with weekly frequency.\n\ndate_range = pd.date_range(start='2023-01-01', periods=10, freq='W')\ndate_range\n\nDatetimeIndex(['2023-01-01', '2023-01-08', '2023-01-15', '2023-01-22',\n               '2023-01-29', '2023-02-05', '2023-02-12', '2023-02-19',\n               '2023-02-26', '2023-03-05'],\n              dtype='datetime64[ns]', freq='W-SUN')\n\n\n\n\nCreating Monthly Date Ranges\nGenerate a sequence of dates with monthly frequency using ‘ME’ (Month End).\n\ndate_range = pd.date_range(start='2023-01-01', periods=50, freq='ME')\ndate_range\n\nDatetimeIndex(['2023-01-31', '2023-02-28', '2023-03-31', '2023-04-30',\n               '2023-05-31', '2023-06-30', '2023-07-31', '2023-08-31',\n               '2023-09-30', '2023-10-31', '2023-11-30', '2023-12-31',\n               '2024-01-31', '2024-02-29', '2024-03-31', '2024-04-30',\n               '2024-05-31', '2024-06-30', '2024-07-31', '2024-08-31',\n               '2024-09-30', '2024-10-31', '2024-11-30', '2024-12-31',\n               '2025-01-31', '2025-02-28', '2025-03-31', '2025-04-30',\n               '2025-05-31', '2025-06-30', '2025-07-31', '2025-08-31',\n               '2025-09-30', '2025-10-31', '2025-11-30', '2025-12-31',\n               '2026-01-31', '2026-02-28', '2026-03-31', '2026-04-30',\n               '2026-05-31', '2026-06-30', '2026-07-31', '2026-08-31',\n               '2026-09-30', '2026-10-31', '2026-11-30', '2026-12-31',\n               '2027-01-31', '2027-02-28'],\n              dtype='datetime64[ns]', freq='ME')"
  },
  {
    "objectID": "posts/pandas/Handling Temporal Datas.html#summary",
    "href": "posts/pandas/Handling Temporal Datas.html#summary",
    "title": "Handling Temporal Data with Pandas",
    "section": "Summary",
    "text": "Summary\nIn this notebook, you learned comprehensive techniques for handling temporal data in Pandas:\n\n🔧 Datetime Conversion & Manipulation\n\nConvert string dates to datetime objects with pd.to_datetime()\nExtract date components (year, month, day) using .dt accessor\nHandle different date formats and timezones\n\n\n\n📊 Time Series Operations\n\nCreate time series data with datetime indexing\nResample data to different frequencies (daily → weekly, etc.)\nSet datetime columns as DataFrame index for time-based operations\n\n\n\n⏱️ Time Series Analysis\n\nUse shift() for creating lag/lead features\nPerform period-over-period comparisons\nHandle time-based data transformations\n\n\n\n📅 Date Range Generation\n\nCreate sequences of dates with pd.date_range()\nSpecify different frequencies (daily, weekly, monthly)\nGenerate date ranges for filling missing data or creating time indices\n\n\n\n🚀 Key Takeaways\n\nAlways convert dates: Use pd.to_datetime() for proper date handling\nSet datetime index: For time series analysis, make dates the index\nUse .dt accessor: Extract components like year, month, day easily\nMaster shifting: shift() is essential for time series features\nResample wisely: Change data frequency based on your analysis needs\n\n\n\n📈 Next Steps\n\nPractice with real time series datasets (stock prices, weather data, etc.)\nExplore advanced topics like timezones and business day calculations\nLearn about rolling windows and expanding operations\nCombine temporal data with other Pandas operations for comprehensive analysis\n\nTemporal data is everywhere - master these techniques and you’ll be equipped to handle any time-based analysis! 🕐📊"
  },
  {
    "objectID": "posts/pandas/Group By Operations.html",
    "href": "posts/pandas/Group By Operations.html",
    "title": "Group By Operations in Pandas",
    "section": "",
    "text": "GroupBy operations are one of the most powerful features in Pandas for data analysis. They allow you to:\nThis notebook covers essential groupby techniques including aggregation functions, multiple aggregations, and advanced operations."
  },
  {
    "objectID": "posts/pandas/Group By Operations.html#setting-up-sample-data",
    "href": "posts/pandas/Group By Operations.html#setting-up-sample-data",
    "title": "Group By Operations in Pandas",
    "section": "1. Setting Up Sample Data",
    "text": "1. Setting Up Sample Data\nLet’s create a sample dataset to demonstrate groupby operations. We’ll work with categorical data and numerical values.\n\nimport pandas as pd\n\ndata = {\n    'Category': ['A', 'B', 'A', 'B', 'A'],\n    'Value': [10,15,20,25,30]\n}\n\ndf = pd.DataFrame(data)"
  },
  {
    "objectID": "posts/pandas/Group By Operations.html#basic-aggregation-functions",
    "href": "posts/pandas/Group By Operations.html#basic-aggregation-functions",
    "title": "Group By Operations in Pandas",
    "section": "2. Basic Aggregation Functions",
    "text": "2. Basic Aggregation Functions\nGroupby operations allow you to calculate summary statistics for each group. Here are the most common aggregation functions:\n\nSum Aggregation\nCalculate the total sum of values for each category:\n\ndf.groupby('Category').sum()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n60\n\n\nB\n40\n\n\n\n\n\n\n\n\n\nMean Aggregation\nCalculate the average value for each category:\n\ndf.groupby('Category').mean()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n20.0\n\n\nB\n20.0\n\n\n\n\n\n\n\n\n\nMedian Aggregation\nCalculate the median (middle) value for each category:\n\ndf.groupby('Category').median()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n20.0\n\n\nB\n20.0\n\n\n\n\n\n\n\n\n\nMaximum Values\nFind the highest value in each category:\n\ndf.groupby('Category').max()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n30\n\n\nB\n25\n\n\n\n\n\n\n\n\n\nMinimum Values\nFind the lowest value in each category:\n\ndf.groupby('Category').min()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n10\n\n\nB\n15\n\n\n\n\n\n\n\n\n\nStandard Deviation\nMeasure the spread of values within each category:\n\ndf.groupby('Category').std()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n10.000000\n\n\nB\n7.071068\n\n\n\n\n\n\n\n\n\nVariance\nCalculate the variance (squared standard deviation) for each category:\n\ndf.groupby('Category').var()\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n100.0\n\n\nB\n50.0"
  },
  {
    "objectID": "posts/pandas/Group By Operations.html#multiple-aggregations",
    "href": "posts/pandas/Group By Operations.html#multiple-aggregations",
    "title": "Group By Operations in Pandas",
    "section": "3. Multiple Aggregations",
    "text": "3. Multiple Aggregations\nYou can apply multiple aggregation functions at once using the agg() method. This provides a comprehensive view of your grouped data.\n\nApplying Multiple Functions\nCalculate sum, mean, and maximum for each category in one operation:\n\ndf.groupby('Category').agg(['sum', 'mean', 'max'])\n\n\n\n\n\n\n\n\nValue\n\n\n\nsum\nmean\nmax\n\n\nCategory\n\n\n\n\n\n\n\nA\n60\n20.0\n30\n\n\nB\n40\n20.0\n25"
  },
  {
    "objectID": "posts/pandas/Group By Operations.html#summary",
    "href": "posts/pandas/Group By Operations.html#summary",
    "title": "Group By Operations in Pandas",
    "section": "Summary",
    "text": "Summary\nGroupBy operations are essential for data analysis in Pandas. In this notebook, you learned:\n\n🔢 Basic Aggregation Functions\n\nsum(): Total values per group\nmean(): Average values per group\n\nmedian(): Middle value per group\nmax() / min(): Highest/lowest values per group\nstd() / var(): Measure spread within groups\n\n\n\n📊 Advanced Operations\n\nagg(): Apply multiple functions simultaneously\nCombine statistics for comprehensive group analysis\n\n\n\n💡 Key Concepts\n\nSplit-Apply-Combine: The three-step process of groupby operations\nAggregation: Reducing groups to single values (sum, mean, etc.)\nMultiple Functions: Use agg() for comprehensive summaries\n\n\n\n🚀 Best Practices\n\nChoose appropriate aggregation functions for your data type\nUse multiple aggregations to get complete group insights\nConsider data distribution when selecting measures (mean vs median)\n\n\n\n📈 Next Steps\n\nExplore groupby with multiple columns\nLearn filtering and transformation operations\nPractice with real datasets for business insights\n\nMastering groupby operations will significantly enhance your data analysis capabilities! 🎯"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html",
    "href": "posts/pandas/data manipulation with loc and iloc.html",
    "title": "Data Manipulation with loc and iloc",
    "section": "",
    "text": "Import Pandas and NumPy for data manipulation.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#import-libraries",
    "href": "posts/pandas/data manipulation with loc and iloc.html#import-libraries",
    "title": "Data Manipulation with loc and iloc",
    "section": "",
    "text": "Import Pandas and NumPy for data manipulation.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#create-dataframe-with-custom-index",
    "href": "posts/pandas/data manipulation with loc and iloc.html#create-dataframe-with-custom-index",
    "title": "Data Manipulation with loc and iloc",
    "section": "Create DataFrame with Custom Index",
    "text": "Create DataFrame with Custom Index\nCreate a DataFrame with columns A, B, C and custom row labels (‘a’, ‘b’, ‘c’, ‘d’, ‘e’).\n\ndata = {'A': [1,2,3,4,5],\n        'B': [6,7,8,9,10],\n        'C': [11,12,13,14,15]\n        }\ndf = pd.DataFrame(data, index=['a','b','c','d','e'])\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nb\n2\n7\n12\n\n\nc\n3\n8\n13\n\n\nd\n4\n9\n14\n\n\ne\n5\n10\n15"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-rows-by-label-range-with-loc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-rows-by-label-range-with-loc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Rows by Label Range with loc",
    "text": "Select Rows by Label Range with loc\nUse df.loc[\"a\":'c'] to select rows from label ‘a’ to ‘c’ (inclusive).\n\ndf.loc[\"a\":'c']\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nb\n2\n7\n12\n\n\nc\n3\n8\n13"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-by-label-with-loc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-by-label-with-loc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Specific Rows by Label with loc",
    "text": "Select Specific Rows by Label with loc\nUse df.loc[['a','c']] to select rows with labels ‘a’ and ‘c’.\n\ndf.loc[['a','c']]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nc\n3\n8\n13"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-and-columns-by-label-with-loc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-and-columns-by-label-with-loc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Specific Rows and Columns by Label with loc",
    "text": "Select Specific Rows and Columns by Label with loc\nUse df.loc[['a','c'],['A','C']] to select rows ‘a’ and ‘c’ and columns ‘A’ and ‘C’.\n\ndf.loc[['a','c'],['A','C']]\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\na\n1\n11\n\n\nc\n3\n13"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#display-the-dataframe",
    "href": "posts/pandas/data manipulation with loc and iloc.html#display-the-dataframe",
    "title": "Data Manipulation with loc and iloc",
    "section": "Display the DataFrame",
    "text": "Display the DataFrame\nShow the entire DataFrame for reference.\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nb\n2\n7\n12\n\n\nc\n3\n8\n13\n\n\nd\n4\n9\n14\n\n\ne\n5\n10\n15"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-row-by-integer-position-with-iloc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-row-by-integer-position-with-iloc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Row by Integer Position with iloc",
    "text": "Select Row by Integer Position with iloc\nUse df.iloc[0] to select the first row by its integer position.\n\ndf.iloc[0]\n\nA     1\nB     6\nC    11\nName: a, dtype: int64"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-multiple-rows-by-integer-range-with-iloc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-multiple-rows-by-integer-range-with-iloc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Multiple Rows by Integer Range with iloc",
    "text": "Select Multiple Rows by Integer Range with iloc\nUse df.iloc[0:3] to select rows from position 0 to 2 (Python slicing is exclusive of the end).\n\ndf.iloc[0:3]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nb\n2\n7\n12\n\n\nc\n3\n8\n13"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-rows-from-a-position-to-end-with-iloc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-rows-from-a-position-to-end-with-iloc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Rows from a Position to End with iloc",
    "text": "Select Rows from a Position to End with iloc\nUse df.iloc[3:] to select all rows from position 3 to the end.\n\ndf.iloc[3:]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\nd\n4\n9\n14\n\n\ne\n5\n10\n15"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-by-integer-position-with-iloc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-by-integer-position-with-iloc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Specific Rows by Integer Position with iloc",
    "text": "Select Specific Rows by Integer Position with iloc\nUse df.iloc[[0,3]] to select rows at positions 0 and 3.\n\ndf.iloc[[0,3]]\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\na\n1\n6\n11\n\n\nd\n4\n9\n14"
  },
  {
    "objectID": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-and-columns-by-integer-position-with-iloc",
    "href": "posts/pandas/data manipulation with loc and iloc.html#select-specific-rows-and-columns-by-integer-position-with-iloc",
    "title": "Data Manipulation with loc and iloc",
    "section": "Select Specific Rows and Columns by Integer Position with iloc",
    "text": "Select Specific Rows and Columns by Integer Position with iloc\nUse df.iloc[[0,3],[0,2]] to select rows at positions 0 and 3, and columns at positions 0 and 2.\n\ndf.iloc[[0,3],[0,2]]\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\na\n1\n11\n\n\nd\n4\n14"
  },
  {
    "objectID": "posts/pandas/Combine and Merge DF.html",
    "href": "posts/pandas/Combine and Merge DF.html",
    "title": "Combining and Merging DataFrames with Pandas",
    "section": "",
    "text": "This notebook is a guide to combining and merging DataFrames in Pandas, two fundamental operations for data wrangling. We’ll explore: - pd.concat(): For stacking DataFrames vertically or horizontally. - pd.merge(): For performing database-style joins on DataFrames."
  },
  {
    "objectID": "posts/pandas/Combine and Merge DF.html#setting-up-sample-dataframes",
    "href": "posts/pandas/Combine and Merge DF.html#setting-up-sample-dataframes",
    "title": "Combining and Merging DataFrames with Pandas",
    "section": "1. Setting Up Sample DataFrames",
    "text": "1. Setting Up Sample DataFrames\nFirst, let’s import Pandas and create a few sample DataFrames to work with.\n\nimport pandas as pd\n\ndata1 = {\n    'A': [1,2,3],\n    'B': [4,5,6]\n}\n\ndata2 = {\n    'A': [7,8,9],   \n    'B': [10,11,12]\n}\n\ndf1 = pd.DataFrame(data1)\ndf2 = pd.DataFrame(data2)\n\n\ndf1\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n7\n10\n\n\n1\n8\n11\n\n\n2\n9\n12"
  },
  {
    "objectID": "posts/pandas/Combine and Merge DF.html#concatenating-dataframes-with-pd.concat",
    "href": "posts/pandas/Combine and Merge DF.html#concatenating-dataframes-with-pd.concat",
    "title": "Combining and Merging DataFrames with Pandas",
    "section": "2. Concatenating DataFrames with pd.concat()",
    "text": "2. Concatenating DataFrames with pd.concat()\nConcatenation is like stacking DataFrames on top of each other (row-wise) or side-by-side (column-wise).\n\nDefault Concatenation (Row-wise)\nBy default, pd.concat() stacks DataFrames vertically. This is useful when you have DataFrames with the same columns.\n\npd.concat([df1, df2])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n0\n7\n10\n\n\n1\n8\n11\n\n\n2\n9\n12\n\n\n\n\n\n\n\n\n\nColumn-wise Concatenation\nYou can also concatenate side-by-side by setting axis=1. This is useful for adding new columns.\n\npd.concat([df1, df2], axis=1)\n\n\n\n\n\n\n\n\nA\nB\nA\nB\n\n\n\n\n0\n1\n4\n7\n10\n\n\n1\n2\n5\n8\n11\n\n\n2\n3\n6\n9\n12"
  },
  {
    "objectID": "posts/pandas/Combine and Merge DF.html#merging-dataframes-with-pd.merge",
    "href": "posts/pandas/Combine and Merge DF.html#merging-dataframes-with-pd.merge",
    "title": "Combining and Merging DataFrames with Pandas",
    "section": "3. Merging DataFrames with pd.merge()",
    "text": "3. Merging DataFrames with pd.merge()\nMerging is used for database-style joins. It combines DataFrames based on a common column (a “key”).\n\nDefault Merge (Inner Join)\npd.merge() performs an inner join by default, combining rows that have matching values in the specified on column.\n\ndata3 = {\n    'A': [1,2,3],   \n    'C': [13,14,15]\n}\n\ndf3 = pd.DataFrame(data3)\n\ndf3\n\n\n\n\n\n\n\n\nA\nC\n\n\n\n\n0\n1\n13\n\n\n1\n2\n14\n\n\n2\n3\n15\n\n\n\n\n\n\n\n\ndf1\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n3\n6\n\n\n\n\n\n\n\n\npd.merge(df1, df3, on='A')\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n4\n13\n\n\n1\n2\n5\n14\n\n\n2\n3\n6\n15"
  },
  {
    "objectID": "posts/pandas/Combine and Merge DF.html#summary",
    "href": "posts/pandas/Combine and Merge DF.html#summary",
    "title": "Combining and Merging DataFrames with Pandas",
    "section": "Summary",
    "text": "Summary\nIn this notebook, you learned the key differences between concat and merge:\n\npd.concat(): Best for stacking DataFrames.\n\naxis=0 (default): Stacks vertically (appends rows).\naxis=1: Stacks horizontally (appends columns).\n\npd.merge(): Best for database-style joins based on common columns.\n\nUse the on parameter to specify the key to join on.\nSupports inner, outer, left, and right joins (though we only covered the default inner join here).\n\n\nUnderstanding when to use each is crucial for effective data manipulation in Pandas. Happy coding! 🚀"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html",
    "href": "posts/pandas/Advanced Data Manipulation.html",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "",
    "text": "This notebook demonstrates advanced element-wise and row/column-wise transformations in pandas using apply, map, and applymap."
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#introduction",
    "href": "posts/pandas/Advanced Data Manipulation.html#introduction",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Introduction",
    "text": "Introduction\nPandas provides flexible methods to transform data: - Series.map(func): elementwise mapping for a Series. - DataFrame.apply(func, axis=...): apply a function to each column or row (as Series). - DataFrame.applymap(func): elementwise operation across the entire DataFrame.\nWe’ll illustrate each with short examples and best-practice notes.\n\n# Import libraries and create sample DataFrame\nimport pandas as pd\n\ndata = {\n    'A': [1, 2, 3, 4, 5],\n    'B': [10, 20, 30, 40, 50],\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n10\n\n\n1\n2\n20\n\n\n2\n3\n30\n\n\n3\n4\n40\n\n\n4\n5\n50"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#using-apply",
    "href": "posts/pandas/Advanced Data Manipulation.html#using-apply",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Using apply",
    "text": "Using apply\nDataFrame.apply calls a function on each column (by default) or each row when axis=1. The function receives a Series and should return a single value or a Series (for aggregation or transformation).\nUse apply when your operation needs to work on an entire row/column at once (e.g., compute a statistic or combine multiple columns).\n\n# Example: multiply each column (Series) by 2 using apply\n# Note: apply receives a Series (column) by default, so multiplying the Series scales all values in that column\ndf_apply = df.apply(lambda col: col * 2)\ndf_apply\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n2\n20\n\n\n1\n4\n40\n\n\n2\n6\n60\n\n\n3\n8\n80\n\n\n4\n10\n100"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#using-map-series",
    "href": "posts/pandas/Advanced Data Manipulation.html#using-map-series",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Using map (Series)",
    "text": "Using map (Series)\nSeries.map is an elementwise operation on a Series. Use it for simple scalar transformations or to map values via a dict/Series/function. It is not available on DataFrame directly (use applymap for elementwise on DataFrame).\n\n# Series example using map\nseries_data = pd.Series([1, 2, 3, 4, 5])\nmapped_data = series_data.map(lambda x: x ** 2)\nmapped_data\n\n0     1\n1     4\n2     9\n3    16\n4    25\ndtype: int64\n\n\n\n# original series\nseries_data\n\n0    1\n1    2\n2    3\n3    4\n4    5\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#using-applymap-elementwise-on-dataframe",
    "href": "posts/pandas/Advanced Data Manipulation.html#using-applymap-elementwise-on-dataframe",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Using applymap (elementwise on DataFrame)",
    "text": "Using applymap (elementwise on DataFrame)\nDataFrame.applymap applies a function to each element of the DataFrame. This is the correct choice for elementwise numeric transforms across all cells. For column/row-wise operations, prefer apply.\n\n# show the DataFrame\ndf\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n10\n\n\n1\n2\n20\n\n\n2\n3\n30\n\n\n3\n4\n40\n\n\n4\n5\n50\n\n\n\n\n\n\n\n\n# elementwise cube using applymap\ndf_applymap = df.applymap(lambda x: x ** 3)\ndf_applymap\n\nC:\\Users\\adila\\AppData\\Local\\Temp\\ipykernel_5712\\4130872161.py:2: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n  df_applymap = df.applymap(lambda x: x ** 3)\n\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1000\n\n\n1\n8\n8000\n\n\n2\n27\n27000\n\n\n3\n64\n64000\n\n\n4\n125\n125000"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#creating-new-columns-with-apply-row-wise",
    "href": "posts/pandas/Advanced Data Manipulation.html#creating-new-columns-with-apply-row-wise",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Creating new columns with apply (row-wise)",
    "text": "Creating new columns with apply (row-wise)\nWhen you need to compute a value using multiple columns, use apply with axis=1. For better performance, prefer vectorized operations when possible (see Best Practices below).\n\n# create column 'C' as product of A and B using apply row-wise\ndf['C'] = df.apply(lambda row: row['A'] * row['B'], axis=1)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n10\n10\n\n\n1\n2\n20\n40\n\n\n2\n3\n30\n90\n\n\n3\n4\n40\n160\n\n\n4\n5\n50\n250"
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#best-practices",
    "href": "posts/pandas/Advanced Data Manipulation.html#best-practices",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Best Practices",
    "text": "Best Practices\n\nPrefer pandas vectorized operations (e.g., df['A'] * df['B']) over apply when possible — they are faster and clearer.\nUse map for Series-to-Series elementwise mappings or label replacements.\nUse applymap only when you need a uniform elementwise transform across the entire DataFrame.\nWhen using apply with axis=1, consider np.where, pd.Series.where, or vectorized arithmetic to improve performance.\nKeep functions simple and avoid expensive Python-level loops inside apply/map for large DataFrames."
  },
  {
    "objectID": "posts/pandas/Advanced Data Manipulation.html#summary-further-reading",
    "href": "posts/pandas/Advanced Data Manipulation.html#summary-further-reading",
    "title": "Advanced Data Manipulation (apply, map, applymap)",
    "section": "Summary & Further Reading",
    "text": "Summary & Further Reading\nThis notebook covered the differences between map, apply, and applymap and showed practical examples. For more, see the pandas documentation: https://pandas.pydata.org/pandas-docs/stable/reference/index.html\nFurther exercises: try rewriting the df['C'] calculation using a fully vectorized expression and compare timing with %timeit."
  },
  {
    "objectID": "posts/fastai/textclassification.html",
    "href": "posts/fastai/textclassification.html",
    "title": "TIP",
    "section": "",
    "text": "from transformers import pipeline\n\n\nspam_classifier = pipeline(\n    'text-classification',\n    model='philschmid/distilbert-base-multilingual-cased-sentiment'\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevice set to use cuda:0\n\n\n\ndocs = [\n    \"Congartulations! You've won 5oo INR Amazon gift voucher\",\n    'Hey Amit. Lets have a meeting tomorrow',\n    'URGENT: Youre gmail has been comprimsed. Click this link to revive it',\n    'URGENT: Youre Credit Card has been comprimsed. Click this link to revive it'\n]\n\nresults = spam_classifier(docs)\n\n\nlabel_mapping = {'negative': 'SPAM',\n                 'neutral':'NOT SPAM',\n                 'positive':'NOT SPAM'}\n\nfor res in results:\n    label = label_mapping[res['label']]\n    score = res['score']\n    print(f\"Label: {label}, Confidence: {score:.2f}\")\n\nLabel: SPAM, Confidence: 0.96\nLabel: NOT SPAM, Confidence: 0.80\nLabel: SPAM, Confidence: 0.92\nLabel: SPAM, Confidence: 0.94"
  },
  {
    "objectID": "posts/fastai/NLP.html",
    "href": "posts/fastai/NLP.html",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "",
    "text": "import os\niskaggle = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', '')\nThis notebook implements a solution for the US Patent Phrase to Phrase Matching competition using DeBERTa-v3-small model. The goal is to predict similarity scores between patent phrase pairs.\n# %pip install kaggle\nimport kaggle"
  },
  {
    "objectID": "posts/fastai/NLP.html#setup-kaggle-environment",
    "href": "posts/fastai/NLP.html#setup-kaggle-environment",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Setup Kaggle Environment",
    "text": "Setup Kaggle Environment\nSetting up Kaggle credentials and API access for downloading competition data. We create a credentials file and set appropriate permissions.\n\ncreds = '{\"username\":\"mohammedadilsiraju\",\"key\":\"572884063344e3ecff7b5e73ace34c3c\"}'\n\n\n# for working with paths in Python, I recommend using `pathlib.Path`\nfrom pathlib import Path\n\ncred_path = Path('~/.kaggle/kaggle.json').expanduser()\nif not cred_path.exists():\n    cred_path.parent.mkdir(exist_ok=True)\n    cred_path.write_text(creds)\n    cred_path.chmod(0o600)\n\n\n# Download the competition data using Python API\npath = Path('data')\nif not iskaggle and not path.exists():\n    import zipfile\n    kaggle.api.competition_download_cli('us-patent-phrase-to-phrase-matching')\n    zipfile.ZipFile('us-patent-phrase-to-phrase-matching.zip').extractall(path)"
  },
  {
    "objectID": "posts/fastai/NLP.html#import-and-eda",
    "href": "posts/fastai/NLP.html#import-and-eda",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Import and EDA",
    "text": "Import and EDA\n\n# %pip install -q datasets\n\n\n%ls {path}\n\nsample_submission.csv  test.csv  train.csv\n\n\n\nimport pandas as pd\ndf = pd.read_csv(path/'train.csv')"
  },
  {
    "objectID": "posts/fastai/NLP.html#data-loading-and-initial-eda",
    "href": "posts/fastai/NLP.html#data-loading-and-initial-eda",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Data Loading and Initial EDA",
    "text": "Data Loading and Initial EDA\nLoading the training data and performing initial exploratory data analysis to understand the structure and content of our dataset.\n\ndf\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\nscore\n\n\n\n\n0\n37d61fd2272659b1\nabatement\nabatement of pollution\nA47\n0.50\n\n\n1\n7b9652b17b68b7a4\nabatement\nact of abating\nA47\n0.75\n\n\n2\n36d72442aefd8232\nabatement\nactive catalyst\nA47\n0.25\n\n\n3\n5296b0c19e1ce60e\nabatement\neliminating process\nA47\n0.50\n\n\n4\n54c1e3b9184cb5b6\nabatement\nforest region\nA47\n0.00\n\n\n...\n...\n...\n...\n...\n...\n\n\n36468\n8e1386cbefd7f245\nwood article\nwooden article\nB44\n1.00\n\n\n36469\n42d9e032d1cd3242\nwood article\nwooden box\nB44\n0.50\n\n\n36470\n208654ccb9e14fa3\nwood article\nwooden handle\nB44\n0.50\n\n\n36471\n756ec035e694722b\nwood article\nwooden material\nB44\n0.75\n\n\n36472\n8d135da0b55b8c88\nwood article\nwooden substrate\nB44\n0.50\n\n\n\n\n36473 rows × 5 columns\n\n\n\n\ndf.describe(include='object')\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\ncount\n36473\n36473\n36473\n36473\n\n\nunique\n36473\n733\n29340\n106\n\n\ntop\n37d61fd2272659b1\ncomponent composite coating\ncomposition\nH01\n\n\nfreq\n1\n152\n24\n2186\n\n\n\n\n\n\n\n\ndf['input'] = 'TEXT1: ' + df.context + '; TEXT2: ' + df.target + '; ANC1: ' + df.anchor"
  },
  {
    "objectID": "posts/fastai/NLP.html#input-formatting",
    "href": "posts/fastai/NLP.html#input-formatting",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Input Formatting",
    "text": "Input Formatting\nCreating a structured input format by combining context, target, and anchor texts. This format helps the model understand the relationships between different phrases.\n\ndf.input\n\n0        TEXT1: A47; TEXT2: abatement of pollution; ANC...\n1        TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n2        TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n3        TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n4        TEXT1: A47; TEXT2: forest region; ANC1: abatement\n                               ...                        \n36468    TEXT1: B44; TEXT2: wooden article; ANC1: wood ...\n36469    TEXT1: B44; TEXT2: wooden box; ANC1: wood article\n36470    TEXT1: B44; TEXT2: wooden handle; ANC1: wood a...\n36471    TEXT1: B44; TEXT2: wooden material; ANC1: wood...\n36472    TEXT1: B44; TEXT2: wooden substrate; ANC1: woo...\nName: input, Length: 36473, dtype: object\n\n\n\nfrom datasets import Dataset, DatasetDict\n\nds = Dataset.from_pandas(df)\nds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n    num_rows: 36473\n})\n\n\n\nmodel_nm = 'microsoft/deberta-v3-small'\n\n\n# %pip install transformers tiktoken\n\n\n# %pip install -U transformers tokenizers SentencePiece\n\n\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\ntokz = AutoTokenizer.from_pretrained(model_nm)\n\n/home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn("
  },
  {
    "objectID": "posts/fastai/NLP.html#model-setup-and-tokenization",
    "href": "posts/fastai/NLP.html#model-setup-and-tokenization",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Model Setup and Tokenization",
    "text": "Model Setup and Tokenization\nLoading the DeBERTa-v3-small tokenizer and testing it with sample texts to ensure proper tokenization.\n\ntokz.tokenize(\"G'day folks, I'm Adil Siraju from kerala\")\n\n['▁G',\n \"'\",\n 'day',\n '▁folks',\n ',',\n '▁I',\n \"'\",\n 'm',\n '▁Adil',\n '▁Siraj',\n 'u',\n '▁from',\n '▁kerala']\n\n\n\ntokz.tokenize(\"A platypus is an ornithorhynchus anatinus.\")\n\n['▁A',\n '▁platypus',\n '▁is',\n '▁an',\n '▁or',\n 'ni',\n 'tho',\n 'rhynch',\n 'us',\n '▁an',\n 'at',\n 'inus',\n '.']\n\n\n\ndef tok_func(x):\n    return tokz(x['input'])\n\n\ntok_ds = ds.map(tok_func, batched=True)\n\n\n\n\n\nrow = tok_ds[0]\nrow['input'], row['input_ids']\n\n('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n [1,\n  54453,\n  435,\n  294,\n  336,\n  5753,\n  346,\n  54453,\n  445,\n  294,\n  47284,\n  265,\n  6435,\n  346,\n  23702,\n  435,\n  294,\n  47284,\n  2])\n\n\n\ntok_ds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'score', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\n\n\n\ntokz.vocab['▁of']\n\n265\n\n\n\ntok_ds = tok_ds.rename_columns({'score':'labels'})\ntok_ds\n\nDataset({\n    features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 36473\n})\n\n\n\ndds = tok_ds.train_test_split(0.25, seed=42)\ndds\n\nDatasetDict({\n    train: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 27354\n    })\n    test: Dataset({\n        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 9119\n    })\n})\n\n\n\nshow_corr(subset, 'MedHouseVal', 'AveRooms')\n\narray([[1.  , 0.68],\n       [0.68, 1.  ]])\n\n\n\nshow_corr(subset, 'HouseAge', 'AveRooms')\n\n0.6760250732906005\n\n\n\ndef corr_d(eval_pred): return {'pearson': corr(*eval_pred)}\n\ntrain\n\nfrom transformers import Trainer, TrainingArguments"
  },
  {
    "objectID": "posts/fastai/NLP.html#training-configuration",
    "href": "posts/fastai/NLP.html#training-configuration",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Training Configuration",
    "text": "Training Configuration\nSetting up training parameters including batch size, learning rate, and other hyperparameters for fine-tuning the DeBERTa model.\n\nbs = 128\nepochs = 4\n\n\n\n\n\n\n\n\n\nlr = 8e-5\n\n\n\n\n\n\n\n\n\nargs = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n    num_train_epochs=epochs, weight_decay=0.01, report_to='none')\n\n\n\n\n\n\n\n\n\n# %pip install transformers[torch]\n# %pip install 'accelerate&gt;=0.26.0'\n\n\nmodel = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\ntrainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n                  tokenizer=tokz, compute_metrics=corr_d)\n\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_14095/3597993663.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n/tmp/ipykernel_14095/3597993663.py:2: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n\n\n\ntrainer.train()\n\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n\n\n\n      \n      \n      [  11/3424 00:13 &lt; 1:28:24, 0.64 it/s, Epoch 0.02/8]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n\n\nThe tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'eos_token_id': 2, 'bos_token_id': 1}.\n\n\n\n      \n      \n      [  11/3424 00:13 &lt; 1:28:24, 0.64 it/s, Epoch 0.02/8]\n    \n    \n\n\n\nEpoch\nTraining Loss\nValidation Loss\n\n\n\n\n\n\n\n\n\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[57], line 1\n----&gt; 1 trainer.train()\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/trainer.py:2328, in Trainer.train(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\n   2326         hf_hub_utils.enable_progress_bars()\n   2327 else:\n-&gt; 2328     return inner_training_loop(\n   2329         args=args,\n   2330         resume_from_checkpoint=resume_from_checkpoint,\n   2331         trial=trial,\n   2332         ignore_keys_for_eval=ignore_keys_for_eval,\n   2333     )\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/trainer.py:2672, in Trainer._inner_training_loop(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\n   2665 context = (\n   2666     functools.partial(self.accelerator.no_sync, model=model)\n   2667     if i != len(batch_samples) - 1\n   2668     and self.accelerator.distributed_type != DistributedType.DEEPSPEED\n   2669     else contextlib.nullcontext\n   2670 )\n   2671 with context():\n-&gt; 2672     tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n   2674 if (\n   2675     args.logging_nan_inf_filter\n   2676     and not is_torch_xla_available()\n   2677     and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))\n   2678 ):\n   2679     # if loss is nan or inf simply add the average of previous logged losses\n   2680     tr_loss = tr_loss + tr_loss / (1 + self.state.global_step - self._globalstep_last_logged)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/trainer.py:4009, in Trainer.training_step(self, model, inputs, num_items_in_batch)\n   4006     return loss_mb.reduce_mean().detach().to(self.args.device)\n   4008 with self.compute_loss_context_manager():\n-&gt; 4009     loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)\n   4011 del inputs\n   4012 if (\n   4013     self.args.torch_empty_cache_steps is not None\n   4014     and self.state.global_step % self.args.torch_empty_cache_steps == 0\n   4015 ):\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/trainer.py:4099, in Trainer.compute_loss(self, model, inputs, return_outputs, num_items_in_batch)\n   4097         kwargs[\"num_items_in_batch\"] = num_items_in_batch\n   4098     inputs = {**inputs, **kwargs}\n-&gt; 4099 outputs = model(**inputs)\n   4100 # Save past state if it exists\n   4101 # TODO: this needs to be fixed and made cleaner later.\n   4102 if self.args.past_index &gt;= 0:\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/modules/module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1771     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1772 else:\n-&gt; 1773     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/modules/module.py:1784, in Module._call_impl(self, *args, **kwargs)\n   1779 # If we don't have any hooks, we want to skip the rest of the logic in\n   1780 # this function, and just call forward.\n   1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1782         or _global_backward_pre_hooks or _global_backward_hooks\n   1783         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1784     return forward_call(*args, **kwargs)\n   1786 result = None\n   1787 called_always_called_hooks = set()\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/accelerate/utils/operations.py:818, in convert_outputs_to_fp32.&lt;locals&gt;.forward(*args, **kwargs)\n    817 def forward(*args, **kwargs):\n--&gt; 818     return model_forward(*args, **kwargs)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/accelerate/utils/operations.py:806, in ConvertOutputsToFp32.__call__(self, *args, **kwargs)\n    805 def __call__(self, *args, **kwargs):\n--&gt; 806     return convert_to_fp32(self.model_forward(*args, **kwargs))\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/amp/autocast_mode.py:44, in autocast_decorator.&lt;locals&gt;.decorate_autocast(*args, **kwargs)\n     41 @functools.wraps(func)\n     42 def decorate_autocast(*args, **kwargs):\n     43     with autocast_instance:\n---&gt; 44         return func(*args, **kwargs)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1079, in DebertaV2ForSequenceClassification.forward(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\n   1071 r\"\"\"\n   1072 labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n   1073     Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n   1074     config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n   1075     `config.num_labels &gt; 1` a classification loss is computed (Cross-Entropy).\n   1076 \"\"\"\n   1077 return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-&gt; 1079 outputs = self.deberta(\n   1080     input_ids,\n   1081     token_type_ids=token_type_ids,\n   1082     attention_mask=attention_mask,\n   1083     position_ids=position_ids,\n   1084     inputs_embeds=inputs_embeds,\n   1085     output_attentions=output_attentions,\n   1086     output_hidden_states=output_hidden_states,\n   1087     return_dict=return_dict,\n   1088 )\n   1090 encoder_layer = outputs[0]\n   1091 pooled_output = self.pooler(encoder_layer)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/modules/module.py:1773, in Module._wrapped_call_impl(self, *args, **kwargs)\n   1771     return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]\n   1772 else:\n-&gt; 1773     return self._call_impl(*args, **kwargs)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/torch/nn/modules/module.py:1784, in Module._call_impl(self, *args, **kwargs)\n   1779 # If we don't have any hooks, we want to skip the rest of the logic in\n   1780 # this function, and just call forward.\n   1781 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\n   1782         or _global_backward_pre_hooks or _global_backward_hooks\n   1783         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1784     return forward_call(*args, **kwargs)\n   1786 result = None\n   1787 called_always_called_hooks = set()\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:818, in DebertaV2Model.forward(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\n    815 if not return_dict:\n    816     return (sequence_output,) + encoder_outputs[(1 if output_hidden_states else 2) :]\n--&gt; 818 return BaseModelOutput(\n    819     last_hidden_state=sequence_output,\n    820     hidden_states=encoder_outputs.hidden_states if output_hidden_states else None,\n    821     attentions=encoder_outputs.attentions,\n    822 )\n\nFile &lt;string&gt;:6, in __init__(self, last_hidden_state, hidden_states, attentions)\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/utils/generic.py:392, in ModelOutput.__post_init__(self)\n    389 first_field = getattr(self, class_fields[0].name)\n    390 other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])\n--&gt; 392 if other_fields_are_none and not is_tensor(first_field):\n    393     if isinstance(first_field, dict):\n    394         iterator = first_field.items()\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/utils/generic.py:139, in is_tensor(x)\n    134 \"\"\"\n    135 Tests if `x` is a `torch.Tensor`, `tf.Tensor`, `jaxlib.xla_extension.DeviceArray`, `np.ndarray` or `mlx.array`\n    136 in the order defined by `infer_framework_from_repr`\n    137 \"\"\"\n    138 # This gives us a smart order to test the frameworks with the corresponding tests.\n--&gt; 139 framework_to_test_func = _get_frameworks_and_test_func(x)\n    140 for test_func in framework_to_test_func.values():\n    141     if test_func(x):\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/utils/generic.py:124, in _get_frameworks_and_test_func(x)\n    113 \"\"\"\n    114 Returns an (ordered since we are in Python 3.7+) dictionary framework to test function, which places the framework\n    115 we can guess from the repr first, then Numpy, then the others.\n    116 \"\"\"\n    117 framework_to_test = {\n    118     \"pt\": is_torch_tensor,\n    119     \"tf\": is_tf_tensor,\n   (...)\n    122     \"mlx\": is_mlx_array,\n    123 }\n--&gt; 124 preferred_framework = infer_framework_from_repr(x)\n    125 # We will test this one first, then numpy, then the others.\n    126 frameworks = [] if preferred_framework is None else [preferred_framework]\n\nFile ~/miniconda3/envs/fastai/lib/python3.10/site-packages/transformers/utils/generic.py:99, in infer_framework_from_repr(x)\n     94 def infer_framework_from_repr(x):\n     95     \"\"\"\n     96     Tries to guess the framework of an object `x` from its repr (brittle but will help in `is_tensor` to try the\n     97     frameworks in a smart order, without the need to import the frameworks).\n     98     \"\"\"\n---&gt; 99     representation = str(type(x))\n    100     if representation.startswith(\"&lt;class 'torch.\"):\n    101         return \"pt\"\n\nKeyboardInterrupt: \n\n\n\n\neval_df = pd.read_csv(path/'test.csv')\neval_df\n\n\n\n\n\n\n\n\nid\nanchor\ntarget\ncontext\n\n\n\n\n0\n4112d61851461f60\nopc drum\ninorganic photoconductor drum\nG02\n\n\n1\n09e418c93a776564\nadjust gas flow\naltering gas flow\nF23\n\n\n2\n36baf228038e314b\nlower trunnion\nlower locating\nB60\n\n\n3\n1f37ead645e7f0c8\ncap component\nupper portion\nD06\n\n\n4\n71a5b6ad068d531f\nneural stimulation\nartificial neural network\nH04\n\n\n5\n474c874d0c07bd21\ndry corn\ndry corn starch\nC12\n\n\n6\n442c114ed5c4e3c9\ntunneling capacitor\ncapacitor housing\nG11\n\n\n7\nb8ae62ea5e1d8bdb\nangular contact bearing\ncontact therapy radiation\nB23\n\n\n8\nfaaddaf8fcba8a3f\nproduce liquid hydrocarbons\nproduce a treated stream\nC10\n\n\n9\nae0262c02566d2ce\ndiesel fuel tank\ndiesel fuel tanks\nF02\n\n\n10\na8808e31641e856d\nchemical activity\ndielectric characteristics\nB01\n\n\n11\n16ae4b99d3601e60\ntransmit to platform\ndirect receiving\nH04\n\n\n12\n25c555ca3d5a2092\noil tankers\noil carriers\nB63\n\n\n13\n5203a36c501f1b7c\ngenerate in layer\ngenerate by layer\nG02\n\n\n14\nb9fdc772bb8fd61c\nslip segment\nslip portion\nB22\n\n\n15\n7aa5908a77a7ec24\nel display\nillumination\nG02\n\n\n16\nd19ef3979396d47e\noverflow device\noil filler\nE04\n\n\n17\nfd83613b7843f5e1\nbeam traveling direction\nconcrete beam\nH05\n\n\n18\n2a619016908bfa45\nel display\nelectroluminescent\nC23\n\n\n19\n733979d75f59770d\nequipment unit\npower detection\nH02\n\n\n20\n6546846df17f9800\nhalocarbyl\nhalogen addition reaction\nC07\n\n\n21\n3ff0e7a35015be69\nperfluoroalkyl group\nhydroxy\nA63\n\n\n22\n12ca31f018a2e2b9\nspeed control means\ncontrol loop\nG05\n\n\n23\n03ba802ed4029e4d\narm design\nsteel plate\nF16\n\n\n24\nc404f8b378cbb008\nhybrid bearing\nbearing system\nF04\n\n\n25\n78243984c02a72e4\nend pins\nend days\nA44\n\n\n26\nde51114bc0faec3e\norganic starting\norganic farming\nB61\n\n\n27\n7e3aff857f056bf9\nmake of slabs\nmaking cake\nE04\n\n\n28\n26c3c6dc6174b589\nseal teeth\nteeth whitening\nF01\n\n\n29\nb892011ab2e2cabc\ncarry by platform\ncarry on platform\nB60\n\n\n30\n8247ff562ca185cc\npolls\npooling device\nB21\n\n\n31\nc057aecbba832387\nupper clamp arm\nend visual\nA61\n\n\n32\n9f2279ce667b21dc\nclocked storage\nclocked storage device\nG01\n\n\n33\nb9ea2b06a878df6f\ncoupling factor\nturns impedance\nG01\n\n\n34\n79795133c30ef097\ndifferent conductivity\ncarrier polarity\nH03\n\n\n35\n25522ee5411e63e9\nhybrid bearing\ncorrosion resistant\nF16"
  },
  {
    "objectID": "posts/fastai/NLP.html#prediction-and-submission",
    "href": "posts/fastai/NLP.html#prediction-and-submission",
    "title": "Patent Phrase Matching with DeBERTa",
    "section": "Prediction and Submission",
    "text": "Prediction and Submission\nLoading test data, generating predictions, and creating a submission file in the required format.\n\neval_df['input'] = 'TEXT1: ' + eval_df.context + '; TEXT2: ' + eval_df.target + '; ANC1: ' + eval_df.anchor\n\n\neval_ds = Dataset.from_pandas(eval_df)\n\n\neval_ds = eval_ds.map(tok_func, batched=True)\n\n\n\n\n\n# Now predict using the properly formatted dataset\npreds = trainer.predict(eval_ds).predictions.astype(float)\n\n\n\n\n\npreds = np.clip(preds, 0, 1)\npreds\n\narray([[0.48],\n       [0.82],\n       [0.34],\n       [0.35],\n       [0.  ],\n       [0.43],\n       [0.36],\n       [0.05],\n       [0.09],\n       [1.  ],\n       [0.17],\n       [0.28],\n       [0.67],\n       [0.7 ],\n       [0.79],\n       [0.34],\n       [0.22],\n       [0.03],\n       [0.48],\n       [0.25],\n       [0.34],\n       [0.21],\n       [0.08],\n       [0.16],\n       [0.52],\n       [0.  ],\n       [0.  ],\n       [0.03],\n       [0.  ],\n       [0.68],\n       [0.28],\n       [0.04],\n       [0.71],\n       [0.35],\n       [0.35],\n       [0.15]])\n\n\n\nimport datasets\n\nsubmission = datasets.Dataset.from_dict({\n    'id': eval_df['id'],\n    'score': preds\n})\n\nsubmission.to_csv('submission.csv', index=False)\n\n\n\n\n862"
  },
  {
    "objectID": "posts/fastai/index.html",
    "href": "posts/fastai/index.html",
    "title": "FastAI Course",
    "section": "",
    "text": "Lesson 1 - Image Classification\nLesson 2 - Production and Deployment\n\nLesson 3 - Data Ethics and Validation\nLesson 4 - Natural Language Processing\nLesson 5 - Tabular Data and Collaborative Filtering\nLesson 6 - Advanced Architectures\nLesson 7 - Practical Deep Learning"
  },
  {
    "objectID": "posts/fastai/index.html#course-progress",
    "href": "posts/fastai/index.html#course-progress",
    "title": "FastAI Course",
    "section": "",
    "text": "Lesson 1 - Image Classification\nLesson 2 - Production and Deployment\n\nLesson 3 - Data Ethics and Validation\nLesson 4 - Natural Language Processing\nLesson 5 - Tabular Data and Collaborative Filtering\nLesson 6 - Advanced Architectures\nLesson 7 - Practical Deep Learning"
  },
  {
    "objectID": "posts/fastai/index.html#key-projects",
    "href": "posts/fastai/index.html#key-projects",
    "title": "FastAI Course",
    "section": "Key Projects",
    "text": "Key Projects\n\nImage classifiers with transfer learning\nText generation and sentiment analysis\nRecommendation systems\nComputer vision applications"
  },
  {
    "objectID": "posts/fastai/index.html#available-notebooks",
    "href": "posts/fastai/index.html#available-notebooks",
    "title": "FastAI Course",
    "section": "Available Notebooks",
    "text": "Available Notebooks\nPlace your FastAI notebooks here as you progress through the course!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adil’s AI/ML Journey",
    "section": "",
    "text": "AI/ML Deep Learning Hero Image"
  },
  {
    "objectID": "index.html#what-i-share",
    "href": "index.html#what-i-share",
    "title": "Adil’s AI/ML Journey",
    "section": "What I Share",
    "text": "What I Share\n\n\n📚 Learning\nFastAI course progress and ML/DL discoveries\n🔬 Projects\nComputer vision models and MLOps implementations\n\n💡 Insights\nAlgorithm deep dives and optimization strategies\n🚀 Applications\nReal-world problem solving with AI"
  },
  {
    "objectID": "index.html#recent-achievements",
    "href": "index.html#recent-achievements",
    "title": "Adil’s AI/ML Journey",
    "section": "Recent Achievements",
    "text": "Recent Achievements\n\nArchitectural Style Classifier - 73% accuracy across 25 styles\nCI/CD for ML Models - 30% faster deployment cycles\n\nMLOps Expertise - Docker, Kubernetes, automation\n\n\n\n\n\n“The best way to learn is by doing, and the best way to remember is by documenting.”"
  },
  {
    "objectID": "index.html#latest-posts",
    "href": "index.html#latest-posts",
    "title": "Adil’s AI/ML Journey",
    "section": "Latest Posts 📝",
    "text": "Latest Posts 📝\n\n\nRecent Blog Posts - Follow my AI/ML learning journey, technical insights, and project updates.\n\n\nView All Posts →\n\n\n\n\n\n  \n    \n      \n        \n          FastAI Progress: Computer Vision Fundamentals\n        \n        Building my first image classifier with FastAI and exploring transfer learning concepts.\n        \n          Sep 18, 2025\n          \n            fastai\n            computer-vision\n          \n        \n      \n    \n  \n  \n  \n    \n      \n        \n          Welcome to My AI/ML Learning Journey\n        \n        Starting my documentation journey in AI and Machine Learning with goals and focus areas.\n        \n          Sep 15, 2025\n          \n            welcome\n            learning\n          \n        \n      \n    \n  \n\n\n\n\n\n\n\n\nReady to explore? Check out my learning chapters or dive into the complete blog! �"
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\n\n\n\n\n\nBlog Launch: Complete Quarto-based blog setup\nHomepage: Clean, minimal design with hero image\nAbout Page: Professional profile and background\nBlog Posts: Welcome post and FastAI learning progress\n\n\n\n\n\nPulse Theme: Bootstrap-based theme with custom SCSS\nHero Section: Responsive hero image with proper scaling\nMinimal CSS: Streamlined styling leveraging theme defaults\nMobile-First: Responsive design across all devices\n\n\n\n\n\nWelcome Post: Introduction to learning journey\nFastAI Progress: Computer vision fundamentals and classifier project\nAbout Section: Education, skills, experience, and projects\nProject Highlights: Architectural classifier and MLOps work\n\n\n\n\n\nQuarto Setup: Complete configuration with proper metadata\nGitHub Pages: Automated deployment and hosting\nSEO Optimization: Meta descriptions and structured content\nRepository Structure: Clean organization with documentation\n\n\n\n\n\nComprehensive README with setup instructions\nEnhanced .gitignore for Quarto projects\nMIT License for open source sharing\nContributing guidelines for community engagement\nPosts documentation and structure guidelines\n\n\n\n\n\n\n\n\n\nSearch functionality\nTags system for better content organization\nRSS feed optimization\nComment system integration\nDark/light theme toggle\nReading time estimates\nRelated posts suggestions\n\n\n\n\n\nFastAI course completion posts\nComputer vision project deep dives\nMLOps tutorials and best practices\nTechnical interview preparation content\nOpen source contribution experiences\n\n\nKeep building, keep learning! 🚀"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "Blog Launch: Complete Quarto-based blog setup\nHomepage: Clean, minimal design with hero image\nAbout Page: Professional profile and background\nBlog Posts: Welcome post and FastAI learning progress\n\n\n\n\n\nPulse Theme: Bootstrap-based theme with custom SCSS\nHero Section: Responsive hero image with proper scaling\nMinimal CSS: Streamlined styling leveraging theme defaults\nMobile-First: Responsive design across all devices\n\n\n\n\n\nWelcome Post: Introduction to learning journey\nFastAI Progress: Computer vision fundamentals and classifier project\nAbout Section: Education, skills, experience, and projects\nProject Highlights: Architectural classifier and MLOps work\n\n\n\n\n\nQuarto Setup: Complete configuration with proper metadata\nGitHub Pages: Automated deployment and hosting\nSEO Optimization: Meta descriptions and structured content\nRepository Structure: Clean organization with documentation\n\n\n\n\n\nComprehensive README with setup instructions\nEnhanced .gitignore for Quarto projects\nMIT License for open source sharing\nContributing guidelines for community engagement\nPosts documentation and structure guidelines"
  },
  {
    "objectID": "CHANGELOG.html#upcoming-features",
    "href": "CHANGELOG.html#upcoming-features",
    "title": "Changelog",
    "section": "",
    "text": "Search functionality\nTags system for better content organization\nRSS feed optimization\nComment system integration\nDark/light theme toggle\nReading time estimates\nRelated posts suggestions\n\n\n\n\n\nFastAI course completion posts\nComputer vision project deep dives\nMLOps tutorials and best practices\nTechnical interview preparation content\nOpen source contribution experiences\n\n\nKeep building, keep learning! 🚀"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "AI & ML Engineer with First Class with Distinction in B.E. (AI & ML). Experienced in computer vision, deep learning, and MLOps. Currently pursuing FastAI course and documenting my learning journey."
  },
  {
    "objectID": "about.html#mohammed-adil-siraju",
    "href": "about.html#mohammed-adil-siraju",
    "title": "About",
    "section": "",
    "text": "AI & ML Engineer with First Class with Distinction in B.E. (AI & ML). Experienced in computer vision, deep learning, and MLOps. Currently pursuing FastAI course and documenting my learning journey."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "Education",
    "text": "Education\nB.E. Artificial Intelligence & Machine Learning\nP A College of Engineering, Mangalore | 2021 – 2025\nCGPA: 7.33/10 (First Class with Distinction)"
  },
  {
    "objectID": "about.html#skills",
    "href": "about.html#skills",
    "title": "About",
    "section": "Skills",
    "text": "Skills\n\n\nAI/ML - Computer Vision - Deep Learning\n- Model Deployment - Data Science\nProgramming - Python (Advanced) - SQL - Web Development\n\nTools & Frameworks - PyTorch, FastAI - Docker, Kubernetes - Git, Jenkins - scikit-learn\nCurrent Focus - FastAI Course - MLOps - Real-world Applications"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "Experience",
    "text": "Experience\nAI DevOps Engineer - Intern\nRooman Technology | Sep 2024 – Feb 2025\n\nAutomated ML deployment pipelines using Docker & Kubernetes\nImplemented CI/CD workflows, reducing deployment time by 30%\nDesigned scalable infrastructure for ML systems"
  },
  {
    "objectID": "about.html#featured-projects",
    "href": "about.html#featured-projects",
    "title": "About",
    "section": "Featured Projects",
    "text": "Featured Projects\nArchitectural Style Classifier\nCNN-based classifier for 25 architectural styles with 73% accuracy. Deployed via Gradio for 500+ users.\nEcoVest Platform\nFull-stack sustainable investment platform with impact metrics dashboard.\nNetflix Analysis\nComprehensive EDA providing actionable business insights."
  },
  {
    "objectID": "about.html#certifications",
    "href": "about.html#certifications",
    "title": "About",
    "section": "Certifications",
    "text": "Certifications\n\nCS50x - Harvard University (2025)\nPython for Data Science - IBM (2022)\nComputational Thinking - University of Michigan (2022)\n\n\n\n“Continuous learning and practical application are the keys to mastering AI.”"
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "Thank you for your interest in contributing to my learning journey blog! While this is primarily a personal documentation project, I welcome feedback, suggestions, and discussions.\n\n\n\n\n\nIssues: Report bugs or suggest improvements via GitHub Issues\nDiscussions: Share thoughts on learning approaches or technical content\nCode Review: Suggest improvements to code examples or implementations\n\n\n\n\nIf you find any issues: 1. Check existing issues first 2. Provide clear description and steps to reproduce 3. Include browser/device information if relevant\n\n\n\n\nLearning resource recommendations\nTechnical topic suggestions\nProject ideas or improvements\n\n\n\n\n\n\n\n\nQuarto CLI\nBasic knowledge of Markdown and web development\n\n\n\n\ngit clone https://github.com/adilsiraju/Training-in-Progress.git\ncd Training-in-Progress\nquarto preview\n\n\n\n\nFork the repository\nCreate a feature branch (git checkout -b feature/improvement)\nMake your changes\nTest with quarto preview\nCommit with clear messages\nPush and create a Pull Request\n\n\n\n\n\n\n\n\nFocus on learning journey and technical insights\nInclude practical examples and code snippets\nUse clear, concise language\nAdd relevant categories and descriptions\n\n\n\n\n\nEnsure code is functional and tested\nInclude comments for clarity\nFollow Python/web development best practices\n\n\n\n\n\n\nBe respectful and constructive\nFocus on learning and knowledge sharing\nProvide specific, actionable feedback\nRespect that this is a personal learning blog\n\n\n\n\nFor questions or collaboration ideas: - Email: mohdadilsiraju@gmail.com - GitHub: @adilsiraju - Portfolio: adilsiraju.vercel.app\n\nThank you for being part of this learning journey! 🚀"
  },
  {
    "objectID": "CONTRIBUTING.html#ways-to-contribute",
    "href": "CONTRIBUTING.html#ways-to-contribute",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "Issues: Report bugs or suggest improvements via GitHub Issues\nDiscussions: Share thoughts on learning approaches or technical content\nCode Review: Suggest improvements to code examples or implementations\n\n\n\n\nIf you find any issues: 1. Check existing issues first 2. Provide clear description and steps to reproduce 3. Include browser/device information if relevant\n\n\n\n\nLearning resource recommendations\nTechnical topic suggestions\nProject ideas or improvements"
  },
  {
    "objectID": "CONTRIBUTING.html#technical-contributions",
    "href": "CONTRIBUTING.html#technical-contributions",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "Quarto CLI\nBasic knowledge of Markdown and web development\n\n\n\n\ngit clone https://github.com/adilsiraju/Training-in-Progress.git\ncd Training-in-Progress\nquarto preview\n\n\n\n\nFork the repository\nCreate a feature branch (git checkout -b feature/improvement)\nMake your changes\nTest with quarto preview\nCommit with clear messages\nPush and create a Pull Request"
  },
  {
    "objectID": "CONTRIBUTING.html#content-guidelines",
    "href": "CONTRIBUTING.html#content-guidelines",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "Focus on learning journey and technical insights\nInclude practical examples and code snippets\nUse clear, concise language\nAdd relevant categories and descriptions\n\n\n\n\n\nEnsure code is functional and tested\nInclude comments for clarity\nFollow Python/web development best practices"
  },
  {
    "objectID": "CONTRIBUTING.html#community-guidelines",
    "href": "CONTRIBUTING.html#community-guidelines",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "Be respectful and constructive\nFocus on learning and knowledge sharing\nProvide specific, actionable feedback\nRespect that this is a personal learning blog"
  },
  {
    "objectID": "CONTRIBUTING.html#contact",
    "href": "CONTRIBUTING.html#contact",
    "title": "Contributing to TIP - Training in Progress",
    "section": "",
    "text": "For questions or collaboration ideas: - Email: mohdadilsiraju@gmail.com - GitHub: @adilsiraju - Portfolio: adilsiraju.vercel.app\n\nThank you for being part of this learning journey! 🚀"
  },
  {
    "objectID": "posts/fastai/hf.html",
    "href": "posts/fastai/hf.html",
    "title": "TIP",
    "section": "",
    "text": "%pip install datasets\n\nRequirement already satisfied: datasets in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (4.1.1)\nRequirement already satisfied: filelock in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy&gt;=1.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow&gt;=21.0.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (21.0.0)\nRequirement already satisfied: dill&lt;0.4.1,&gt;=0.3.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (0.4.0)\nRequirement already satisfied: pandas in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (2.3.1)\nRequirement already satisfied: requests&gt;=2.32.2 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (2.32.4)\nRequirement already satisfied: tqdm&gt;=4.66.3 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess&lt;0.70.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec&lt;=2025.9.0,&gt;=2023.1.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (2025.7.0)\nRequirement already satisfied: huggingface-hub&gt;=0.24.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (0.34.4)\nRequirement already satisfied: packaging in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (3.12.15)\nRequirement already satisfied: aiohappyeyeballs&gt;=2.5.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (2.6.1)\nRequirement already satisfied: aiosignal&gt;=1.4.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (1.4.0)\nRequirement already satisfied: async-timeout&lt;6.0,&gt;=4.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (5.0.1)\nRequirement already satisfied: attrs&gt;=17.3.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (25.3.0)\nRequirement already satisfied: frozenlist&gt;=1.1.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (1.7.0)\nRequirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (6.6.4)\nRequirement already satisfied: propcache&gt;=0.2.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (0.3.2)\nRequirement already satisfied: yarl&lt;2.0,&gt;=1.17.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (1.20.1)\nRequirement already satisfied: typing-extensions&gt;=4.1.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from multidict&lt;7.0,&gt;=4.5-&gt;aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (4.14.1)\nRequirement already satisfied: idna&gt;=2.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from yarl&lt;2.0,&gt;=1.17.0-&gt;aiohttp!=4.0.0a0,!=4.0.0a1-&gt;fsspec[http]&lt;=2025.9.0,&gt;=2023.1.0-&gt;datasets) (3.10)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from huggingface-hub&gt;=0.24.0-&gt;datasets) (1.1.9)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests&gt;=2.32.2-&gt;datasets) (3.4.3)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests&gt;=2.32.2-&gt;datasets) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests&gt;=2.32.2-&gt;datasets) (2025.8.3)\nRequirement already satisfied: python-dateutil&gt;=2.8.2 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from pandas-&gt;datasets) (2.9.0.post0)\nRequirement already satisfied: pytz&gt;=2020.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: tzdata&gt;=2022.7 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from pandas-&gt;datasets) (2025.2)\nRequirement already satisfied: six&gt;=1.5 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from python-dateutil&gt;=2.8.2-&gt;pandas-&gt;datasets) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom datasets import load_dataset\n\nds = load_dataset('imdb')\n\nprint(ds)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['text', 'label'],\n        num_rows: 50000\n    })\n})\n\n\n\nimport pandas as pd\n\ndf = pd.DataFrame(ds['train'])\ndf\n\n\n\n\n\n\n\n\ntext\nlabel\n\n\n\n\n0\nI rented I AM CURIOUS-YELLOW from my video sto...\n0\n\n\n1\n\"I Am Curious: Yellow\" is a risible and preten...\n0\n\n\n2\nIf only to avoid making this type of film in t...\n0\n\n\n3\nThis film was probably inspired by Godard's Ma...\n0\n\n\n4\nOh, brother...after hearing about this ridicul...\n0\n\n\n...\n...\n...\n\n\n24995\nA hit at the time but now better categorised a...\n1\n\n\n24996\nI love this movie like no other. Another time ...\n1\n\n\n24997\nThis film and it's sequel Barry Mckenzie holds...\n1\n\n\n24998\n'The Adventures Of Barry McKenzie' started lif...\n1\n\n\n24999\nThe story centers around Barry McKenzie who mu...\n1\n\n\n\n\n25000 rows × 2 columns\n\n\n\n\n%pip install transformers\n\nRequirement already satisfied: transformers in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (4.56.2)\nRequirement already satisfied: filelock in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub&lt;1.0,&gt;=0.34.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (0.34.4)\nRequirement already satisfied: numpy&gt;=1.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging&gt;=20.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml&gt;=5.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (2025.9.18)\nRequirement already satisfied: requests in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers&lt;=0.23.0,&gt;=0.22.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors&gt;=0.4.3 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm&gt;=4.27 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec&gt;=2023.5.0 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers) (2025.7.0)\nRequirement already satisfied: typing-extensions&gt;=3.7.4.3 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers) (4.14.1)\nRequirement already satisfied: hf-xet&lt;2.0.0,&gt;=1.1.3 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from huggingface-hub&lt;1.0,&gt;=0.34.0-&gt;transformers) (1.1.9)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests-&gt;transformers) (3.4.3)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests-&gt;transformers) (3.10)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests-&gt;transformers) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /home/adil/miniconda3/envs/fastai/lib/python3.10/site-packages (from requests-&gt;transformers) (2025.8.3)\nNote: you may need to restart the kernel to use updated packages.\n\n\n\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel_name = 'bert-base-uncased'\nmodel = AutoModel.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninputs = tokenizer('Hello, Hugging Face!', return_tensors='pt')\noutputs = model(**inputs)\n\nprint(outputs.last_hidden_state.shape)\n\ntorch.Size([1, 7, 768])"
  },
  {
    "objectID": "posts/fastai/medicaldocclassifier.html",
    "href": "posts/fastai/medicaldocclassifier.html",
    "title": "Medical Document Classifier",
    "section": "",
    "text": "This notebook demonstrates how to build a medical document classifier using the Hugging Face transformers library. We’ll classify medical texts into different categories like: - Clinical Notes - Research Papers - Patient Records - Medical Reports - Prescriptions\n\nfrom transformers import pipeline\nfrom datasets import load_dataset\nimport pandas as pd\n\n\n# Load the medical cases classification dataset\ndataset = load_dataset(\"hpe-ai/medical-cases-classification-tutorial\")\n\n# Display basic information about the dataset\nprint(f\"Dataset keys: {list(dataset.keys())}\")\nprint(f\"Train dataset size: {len(dataset['train'])}\")\n\n# Show the structure of the dataset\nprint(\"\\nDataset features:\")\nprint(dataset['train'].features)\n\n# Display first few examples\nprint(\"\\nFirst 3 examples:\")\nfor i in range(3):\n    example = dataset['train'][i]\n    print(f\"\\nExample {i+1}:\")\n    for key, value in example.items():\n        if isinstance(value, str) and len(value) &gt; 200:\n            print(f\"{key}: {value[:200]}...\")\n        else:\n            print(f\"{key}: {value}\")\n\n\n\n\nRepo card metadata block was not found. Setting CardData to empty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset keys: ['train', 'validation', 'test']\nTrain dataset size: 1724\n\nDataset features:\n{'description': Value('string'), 'transcription': Value('string'), 'sample_name': Value('string'), 'medical_specialty': Value('string'), 'keywords': Value('string')}\n\nFirst 3 examples:\n\nExample 1:\ndescription: Pacemaker ICD interrogation.  Severe nonischemic cardiomyopathy with prior ventricular tachycardia.\ntranscription: PROCEDURE NOTE: , Pacemaker ICD interrogation.,HISTORY OF PRESENT ILLNESS: , The patient is a 67-year-old gentleman who was admitted to the hospital.  He has had ICD pacemaker implantation.  This is a...\nsample_name: Pacemaker Interrogation\nmedical_specialty: Cardiovascular / Pulmonary\nkeywords: cardiovascular / pulmonary, cardiomyopathy, ventricular, tachycardia, pacemaker icd interrogation, millivolts, impendence, interrogation, pacemaker,\n\nExample 2:\ndescription: Erythema of the right knee and leg, possible septic knee. Aspiration through the anterolateral portal of knee joint.\ntranscription: PREOPERATIVE DIAGNOSES: , Erythema of the right knee and leg, possible septic knee.,POSTOPERATIVE DIAGNOSES:,  Erythema of the right knee superficial and leg, right septic knee ruled out.,INDICATIONS:...\nsample_name: Aspiration - Knee Joint\nmedical_specialty: Orthopedic\nkeywords: orthopedic, knee and leg, anterolateral portal, emergency department, spinal needle, septic knee, knee joint, knee, emergency, department, gauge, spinal, needle, aspiration, anterolateral, portal, asp...\n\nExample 3:\ndescription: Left cardiac catheterization with selective right and left coronary angiography.   Post infarct angina.\ntranscription: PREOPERATIVE DIAGNOSIS: , Post infarct angina.,TYPE OF PROCEDURE: , Left cardiac catheterization with selective right and left coronary angiography.,PROCEDURE: , After informed consent was obtained, t...\nsample_name: Cardiac Cath & Selective Coronary Angiography\nmedical_specialty: Cardiovascular / Pulmonary\nkeywords: cardiovascular / pulmonary, selective, angiography, post infarct angina, engaged cineangiograms, coronary angiography, hemodynamic monitoring, cardiac catheterization, catheterization, cineangiograms,...\n\n\n\n# Since the dataset already has medical specialties, let's use that for classification\n# We'll demonstrate with a subset and show how to use the actual medical specialty labels\n\n# Let's work with a sample of the dataset for demonstration\nsample_size = 10\ntrain_sample = dataset['train'].select(range(sample_size))\n\n# Extract the medical texts and their actual labels\nmedical_texts = []\nactual_specialties = []\nsample_names = []\n\nfor i in range(sample_size):\n    description = train_sample[i]['description']\n    transcription = train_sample[i]['transcription'][:300]  # First 300 chars of transcription\n    combined_text = f\"{description}. {transcription}\"\n    \n    medical_texts.append(combined_text)\n    actual_specialties.append(train_sample[i]['medical_specialty'])\n    sample_names.append(train_sample[i]['sample_name'])\n\nprint(f\"Selected {len(medical_texts)} medical texts for analysis\")\nprint(f\"Medical specialties found: {set(actual_specialties)}\")\nprint(f\"\\nExample text: {medical_texts[0][:200]}...\")\nprint(f\"Actual specialty: {actual_specialties[0]}\")\nprint(f\"Sample name: {sample_names[0]}\")\n\nSelected 10 medical texts for analysis\nMedical specialties found: {'Neurology', 'Orthopedic', 'Nephrology', 'ENT - Otolaryngology', 'Obstetrics / Gynecology', 'Cardiovascular / Pulmonary', 'Ophthalmology', 'Gastroenterology'}\n\nExample text: Pacemaker ICD interrogation.  Severe nonischemic cardiomyopathy with prior ventricular tachycardia.. PROCEDURE NOTE: , Pacemaker ICD interrogation.,HISTORY OF PRESENT ILLNESS: , The patient is a 67-ye...\nActual specialty: Cardiovascular / Pulmonary\nSample name: Pacemaker Interrogation\n\n\n\n# Display the results in a structured format\nprint(\"Medical Document Classification Results\")\nprint(\"=\" * 60)\n\nfor idx in range(len(medical_texts)):\n    keywords = train_sample[idx]['keywords']\n    keywords_display = keywords[:100] + \"...\" if keywords and len(keywords) &gt; 100 else (keywords or \"No keywords available\")\n    \n    print(f\"\\nDocument {idx + 1}: {sample_names[idx]}\")\n    print(f\"Text: {medical_texts[idx][:150]}...\")\n    print(f\"Actual Medical Specialty: {actual_specialties[idx]}\")\n    print(f\"Keywords: {keywords_display}\")\n    print(\"-\" * 40)\n\n# Show distribution of medical specialties in our sample\nspecialty_counts = {}\nfor specialty in actual_specialties:\n    specialty_counts[specialty] = specialty_counts.get(specialty, 0) + 1\n\nprint(f\"\\nMedical Specialty Distribution in Sample:\")\nfor specialty, count in specialty_counts.items():\n    print(f\"- {specialty}: {count} cases\")\n\n# Show overall dataset statistics\nprint(f\"\\nDataset Overview:\")\nprint(f\"- Total training examples: {len(dataset['train'])}\")\nprint(f\"- Validation examples: {len(dataset['validation'])}\")  \nprint(f\"- Test examples: {len(dataset['test'])}\")\n\n# Get unique medical specialties in the full dataset\nall_specialties = set(dataset['train']['medical_specialty'])\nprint(f\"\\nAll Medical Specialties in Dataset ({len(all_specialties)} total):\")\nfor specialty in sorted(all_specialties):\n    count = dataset['train']['medical_specialty'].count(specialty)\n    print(f\"- {specialty}: {count} cases\")\n\nMedical Document Classification Results\n============================================================\n\nDocument 1: Pacemaker Interrogation\nText: Pacemaker ICD interrogation.  Severe nonischemic cardiomyopathy with prior ventricular tachycardia.. PROCEDURE NOTE: , Pacemaker ICD interrogation.,HI...\nActual Medical Specialty: Cardiovascular / Pulmonary\nKeywords: cardiovascular / pulmonary, cardiomyopathy, ventricular, tachycardia, pacemaker icd interrogation, m...\n----------------------------------------\n\nDocument 2: Aspiration - Knee Joint\nText: Erythema of the right knee and leg, possible septic knee. Aspiration through the anterolateral portal of knee joint.. PREOPERATIVE DIAGNOSES: , Erythe...\nActual Medical Specialty: Orthopedic\nKeywords: orthopedic, knee and leg, anterolateral portal, emergency department, spinal needle, septic knee, kn...\n----------------------------------------\n\nDocument 3: Cardiac Cath & Selective Coronary Angiography\nText: Left cardiac catheterization with selective right and left coronary angiography.   Post infarct angina.. PREOPERATIVE DIAGNOSIS: , Post infarct angina...\nActual Medical Specialty: Cardiovascular / Pulmonary\nKeywords: cardiovascular / pulmonary, selective, angiography, post infarct angina, engaged cineangiograms, cor...\n----------------------------------------\n\nDocument 4: Acute Kidney Failure\nText: Patient with a history of coronary artery disease, hypertension, diabetes, and stage III CKD.. REASON FOR VISIT: , Acute kidney failure.,HISTORY OF PR...\nActual Medical Specialty: Nephrology\nKeywords: No keywords available\n----------------------------------------\n\nDocument 5: Cardiac Consultation - 6\nText: Cardiac evaluation and treatment in a patient who came in the hospital with abdominal pain.. REASON FOR REFERRAL: , Cardiac evaluation and treatment i...\nActual Medical Specialty: Cardiovascular / Pulmonary\nKeywords: No keywords available\n----------------------------------------\n\nDocument 6: Gardnerella Bacterial Vaginosis\nText: Vaginal discharge with a foul odor.. CHIEF COMPLAINT: , Vaginal discharge with a foul odor.,HISTORY OF PRESENT ILLNESS: , This is a 25-year-old Africa...\nActual Medical Specialty: Obstetrics / Gynecology\nKeywords: No keywords available\n----------------------------------------\n\nDocument 7: Diplopia\nText: He awoke one morning and had double vision.  He states when he closed each eye, the double vision dissipated.  The double vision entirely dissipated w...\nActual Medical Specialty: Ophthalmology\nKeywords: No keywords available\n----------------------------------------\n\nDocument 8: Flex Sig - 2\nText: Flexible sigmoidoscopy.  The Olympus video colonoscope then introduced into the rectum and passed by directed vision to the distal descending colon.. ...\nActual Medical Specialty: Gastroenterology\nKeywords: gastroenterology, flexible sigmoidoscopy, flex sig, colonoscope, olympus video colonoscope, rectumNO...\n----------------------------------------\n\nDocument 9: Lumbar Puncture\nText: Possible CSF malignancy.  This is an 83-year-old woman referred for diagnostic lumbar puncture for possible malignancy by Dr. X.   The patient has gra...\nActual Medical Specialty: Neurology\nKeywords: No keywords available\n----------------------------------------\n\nDocument 10: Ear Pain - Drainage\nText: Right ear pain with drainage - otitis media and otorrhea.. CHIEF COMPLAINT:,  Right ear pain with drainage.,HISTORY OF PRESENT ILLNESS:,  This is a 12...\nActual Medical Specialty: ENT - Otolaryngology\nKeywords: ent - otolaryngology, drainage, ear hurting, ear pain, otitis media, otorrhea, ear pain with drainag...\n----------------------------------------\n\nMedical Specialty Distribution in Sample:\n- Cardiovascular / Pulmonary: 3 cases\n- Orthopedic: 1 cases\n- Nephrology: 1 cases\n- Obstetrics / Gynecology: 1 cases\n- Ophthalmology: 1 cases\n- Gastroenterology: 1 cases\n- Neurology: 1 cases\n- ENT - Otolaryngology: 1 cases\n\nDataset Overview:\n- Total training examples: 1724\n- Validation examples: 370\n- Test examples: 370\n\nAll Medical Specialties in Dataset (13 total):\n- Cardiovascular / Pulmonary: 526 cases\n- ENT - Otolaryngology: 53 cases\n- Gastroenterology: 152 cases\n- Hematology - Oncology: 86 cases\n- Nephrology: 45 cases\n- Neurology: 187 cases\n- Neurosurgery: 76 cases\n- Obstetrics / Gynecology: 126 cases\n- Ophthalmology: 45 cases\n- Orthopedic: 289 cases\n- Pediatrics - Neonatal: 51 cases\n- Psychiatry / Psychology: 49 cases\n- Radiology: 39 cases\n- Neurosurgery: 76 cases\n- Obstetrics / Gynecology: 126 cases\n- Ophthalmology: 45 cases\n- Orthopedic: 289 cases\n- Pediatrics - Neonatal: 51 cases\n- Psychiatry / Psychology: 49 cases\n- Radiology: 39 cases\n\n\n\n# Let's demonstrate actual medical document classification using a fine-tuned model\n# For this example, we'll use a clinical BERT model that's better suited for medical texts\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\nimport torch\n\n# Try to use a medical-domain specific model\ntry:\n    # Use a clinical BERT model if available\n    model_name = \"emilyalsentzer/Bio_ClinicalBERT\"\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    \n    # Create a zero-shot classification pipeline for medical specialties\n    classifier = pipeline(\n        \"zero-shot-classification\",\n        model=\"facebook/bart-large-mnli\"\n    )\n    \n    # Define the medical specialties from our dataset as candidate labels\n    medical_specialties = [\n        \"Cardiovascular Pulmonary\",\n        \"Orthopedic\", \n        \"Nephrology\",\n        \"ENT Otolaryngology\",\n        \"Obstetrics Gynecology\",\n        \"Ophthalmology\",\n        \"Gastroenterology\",\n        \"Neurology\",\n        \"Radiology\",\n        \"Psychiatry Psychology\",\n        \"Pediatrics Neonatal\",\n        \"Hematology Oncology\",\n        \"Neurosurgery\"\n    ]\n    \n    # Test classification on first 3 samples\n    print(\"Zero-Shot Medical Document Classification Results\")\n    print(\"=\" * 60)\n    \n    for i in range(3):\n        text_to_classify = medical_texts[i]\n        actual_specialty = actual_specialties[i]\n        \n        # Perform zero-shot classification\n        result = classifier(text_to_classify, medical_specialties)\n        \n        print(f\"\\nDocument {i+1}: {sample_names[i]}\")\n        print(f\"Text: {text_to_classify[:150]}...\")\n        print(f\"Actual Specialty: {actual_specialty}\")\n        print(f\"Predicted Specialty: {result['labels'][0]}\")\n        print(f\"Confidence: {result['scores'][0]:.3f}\")\n        \n        # Show top 3 predictions\n        print(\"Top 3 predictions:\")\n        for j in range(min(3, len(result['labels']))):\n            print(f\"  {j+1}. {result['labels'][j]}: {result['scores'][j]:.3f}\")\n        print(\"-\" * 50)\n        \n    print(\"\\nNote: This is a zero-shot classification example.\")\n    print(\"For better accuracy, you would typically fine-tune a model\")\n    print(\"specifically on the medical cases classification dataset.\")\n    \nexcept Exception as e:\n    print(f\"Note: Advanced classification model not available: {e}\")\n    print(\"The dataset is ready for use with any medical text classification model.\")\n    print(\"You can fine-tune models like Bio_ClinicalBERT, ClinicalBERT, or other medical domain models.\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDevice set to use cuda:0\n\n\nZero-Shot Medical Document Classification Results\n============================================================\n\nDocument 1: Pacemaker Interrogation\nText: Pacemaker ICD interrogation.  Severe nonischemic cardiomyopathy with prior ventricular tachycardia.. PROCEDURE NOTE: , Pacemaker ICD interrogation.,HI...\nActual Specialty: Cardiovascular / Pulmonary\nPredicted Specialty: Radiology\nConfidence: 0.309\nTop 3 predictions:\n  1. Radiology: 0.309\n  2. Cardiovascular Pulmonary: 0.284\n  3. Nephrology: 0.086\n--------------------------------------------------\n\nDocument 1: Pacemaker Interrogation\nText: Pacemaker ICD interrogation.  Severe nonischemic cardiomyopathy with prior ventricular tachycardia.. PROCEDURE NOTE: , Pacemaker ICD interrogation.,HI...\nActual Specialty: Cardiovascular / Pulmonary\nPredicted Specialty: Radiology\nConfidence: 0.309\nTop 3 predictions:\n  1. Radiology: 0.309\n  2. Cardiovascular Pulmonary: 0.284\n  3. Nephrology: 0.086\n--------------------------------------------------\n\nDocument 2: Aspiration - Knee Joint\nText: Erythema of the right knee and leg, possible septic knee. Aspiration through the anterolateral portal of knee joint.. PREOPERATIVE DIAGNOSES: , Erythe...\nActual Specialty: Orthopedic\nPredicted Specialty: Orthopedic\nConfidence: 0.271\nTop 3 predictions:\n  1. Orthopedic: 0.271\n  2. Nephrology: 0.115\n  3. Cardiovascular Pulmonary: 0.109\n--------------------------------------------------\n\nDocument 2: Aspiration - Knee Joint\nText: Erythema of the right knee and leg, possible septic knee. Aspiration through the anterolateral portal of knee joint.. PREOPERATIVE DIAGNOSES: , Erythe...\nActual Specialty: Orthopedic\nPredicted Specialty: Orthopedic\nConfidence: 0.271\nTop 3 predictions:\n  1. Orthopedic: 0.271\n  2. Nephrology: 0.115\n  3. Cardiovascular Pulmonary: 0.109\n--------------------------------------------------\n\nDocument 3: Cardiac Cath & Selective Coronary Angiography\nText: Left cardiac catheterization with selective right and left coronary angiography.   Post infarct angina.. PREOPERATIVE DIAGNOSIS: , Post infarct angina...\nActual Specialty: Cardiovascular / Pulmonary\nPredicted Specialty: Cardiovascular Pulmonary\nConfidence: 0.303\nTop 3 predictions:\n  1. Cardiovascular Pulmonary: 0.303\n  2. Radiology: 0.096\n  3. Neurology: 0.071\n--------------------------------------------------\n\nNote: This is a zero-shot classification example.\nFor better accuracy, you would typically fine-tune a model\nspecifically on the medical cases classification dataset.\n\nDocument 3: Cardiac Cath & Selective Coronary Angiography\nText: Left cardiac catheterization with selective right and left coronary angiography.   Post infarct angina.. PREOPERATIVE DIAGNOSIS: , Post infarct angina...\nActual Specialty: Cardiovascular / Pulmonary\nPredicted Specialty: Cardiovascular Pulmonary\nConfidence: 0.303\nTop 3 predictions:\n  1. Cardiovascular Pulmonary: 0.303\n  2. Radiology: 0.096\n  3. Neurology: 0.071\n--------------------------------------------------\n\nNote: This is a zero-shot classification example.\nFor better accuracy, you would typically fine-tune a model\nspecifically on the medical cases classification dataset."
  },
  {
    "objectID": "posts/fastai/sientiment_analysis.html",
    "href": "posts/fastai/sientiment_analysis.html",
    "title": "TIP",
    "section": "",
    "text": "from transformers import pipeline\n\n\nsentiment_analyzer = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n\n\n\n\n\n\n\n\n\n\nDevice set to use cuda:0\n\n\n\ntexts = [\n    'I love to play and watch cricket',\n    'I hate when virat kohli misses a century'\n]\n\nresults = sentiment_analyzer(texts)\nfor res in results:\n    print(res)\n\n{'label': 'POSITIVE', 'score': 0.9997660517692566}\n{'label': 'NEGATIVE', 'score': 0.9990317821502686}"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Learning Content & Posts",
    "section": "",
    "text": "Welcome to my learning content hub! Here you’ll find all my Jupyter notebooks, tutorials, and learning materials organized by topic areas.\n\n\n\nPython Fundamentals\nCore Python concepts, modules, and best practices\n\n\nFastAI Course\nPractical deep learning with FastAI framework\n\n\n\nPandas\nData manipulation and analysis with Python’s powerful data library"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html",
    "href": "posts/pandas/Agg Data using Functions.html",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "",
    "text": "Data aggregation is a fundamental operation in data analysis that allows you to summarize and analyze data by groups. This notebook covers:\nMastering these techniques will give you powerful tools for data summarization and analysis."
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#setting-up-sample-data",
    "href": "posts/pandas/Agg Data using Functions.html#setting-up-sample-data",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "1. Setting Up Sample Data",
    "text": "1. Setting Up Sample Data\nLet’s create a sample dataset to demonstrate various aggregation techniques. We’ll work with categorical data and numerical values.\n\nimport pandas as pd\n\ndata = {\n    'Category': ['A', 'B', 'A', 'B', 'A'],\n    'Value': [10,15,20,25,30]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nCategory\nValue\n\n\n\n\n0\nA\n10\n\n\n1\nB\n15\n\n\n2\nA\n20\n\n\n3\nB\n25\n\n\n4\nA\n30"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#built-in-aggregation-functions",
    "href": "posts/pandas/Agg Data using Functions.html#built-in-aggregation-functions",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "2. Built-in Aggregation Functions",
    "text": "2. Built-in Aggregation Functions\nPandas provides many built-in aggregation functions that you can use with the agg() method. These are the most common summary statistics.\n\nSum Aggregation\nCalculate the total sum of values for each category:\n\ndf.groupby('Category').agg({'Value':'sum'})\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n60\n\n\nB\n40\n\n\n\n\n\n\n\n\n\nMean Aggregation\nCalculate the average value for each category:\n\ndf.groupby('Category').agg({'Value':'mean'})\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n20.0\n\n\nB\n20.0\n\n\n\n\n\n\n\n\n\nMaximum Value Aggregation\nFind the highest value in each category:\n\ndf.groupby('Category').agg({'Value':'max'})\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n30\n\n\nB\n25"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#custom-aggregation-functions",
    "href": "posts/pandas/Agg Data using Functions.html#custom-aggregation-functions",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "3. Custom Aggregation Functions",
    "text": "3. Custom Aggregation Functions\nSometimes built-in functions aren’t enough. Pandas allows you to create custom aggregation functions using lambda expressions or named functions.\n\nLambda Functions for Custom Aggregation\nCreate a lambda function to calculate the range (max - min) for each category:\n\ncustom_agg = lambda x: x.max() - x.min() \n\n\ndf\n\n\n\n\n\n\n\n\nCategory\nValue\n\n\n\n\n0\nA\n10\n\n\n1\nB\n15\n\n\n2\nA\n20\n\n\n3\nB\n25\n\n\n4\nA\n30\n\n\n\n\n\n\n\n\ndf.groupby('Category').agg(custom_agg)\n# or\ndf.groupby('Category').agg({'Value': custom_agg})\n\n\n\n\n\n\n\n\nValue\n\n\nCategory\n\n\n\n\n\nA\n20\n\n\nB\n10"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#multiple-aggregations",
    "href": "posts/pandas/Agg Data using Functions.html#multiple-aggregations",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "4. Multiple Aggregations",
    "text": "4. Multiple Aggregations\nYou can apply multiple aggregation functions at once to get comprehensive statistics for each group.\n\nApplying Multiple Built-in Functions\nCalculate count, sum, min, max, and mean for each category:\n\ndf.groupby('Category')['Value'].agg(['count', 'sum', 'min', 'max','mean'])\n\n\n\n\n\n\n\n\ncount\nsum\nmin\nmax\nmean\n\n\nCategory\n\n\n\n\n\n\n\n\n\nA\n3\n60\n10\n30\n20.0\n\n\nB\n2\n40\n15\n25\n20.0"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#named-custom-functions",
    "href": "posts/pandas/Agg Data using Functions.html#named-custom-functions",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "5. Named Custom Functions",
    "text": "5. Named Custom Functions\nFor more complex logic, you can define named functions and use them in aggregations.\n\nCreating a Custom Mean Function\nDefine a function to calculate mean (demonstrating how custom functions work):\n\ndef custom_mean(values):\n    return sum(values) / len(values)\n\ndf.groupby('Category')['Value'].agg(custom_mean)\n\nCategory\nA    20.0\nB    20.0\nName: Value, dtype: float64"
  },
  {
    "objectID": "posts/pandas/Agg Data using Functions.html#summary",
    "href": "posts/pandas/Agg Data using Functions.html#summary",
    "title": "Advanced Data Aggregation with Pandas Functions",
    "section": "Summary",
    "text": "Summary\nData aggregation is a powerful tool for summarizing and analyzing grouped data. In this notebook, you learned:\n\n🔧 Built-in Functions\n\nsum, mean, max: Standard statistical aggregations\nDictionary syntax: agg({'column': 'function'})\nMultiple functions: agg(['func1', 'func2'])\n\n\n\n🎯 Custom Functions\n\nLambda functions: Quick, inline custom logic\nNamed functions: Complex logic with reusable functions\nFlexible application: Apply to specific columns or entire groups\n\n\n\n💡 Key Concepts\n\nDictionary Aggregation: Specify different functions for different columns\nList Aggregation: Apply multiple functions to the same column\nCustom Logic: Create domain-specific aggregations\n\n\n\n🚀 Best Practices\n\nUse built-in functions when possible (more efficient)\nLambda functions for simple custom logic\nNamed functions for complex, reusable operations\nChoose appropriate aggregations based on your data and analysis goals\n\n\n\n📊 Next Steps\n\nExplore groupby with multiple columns\nLearn about transformation and filtering operations\nPractice with real datasets to create meaningful aggregations\n\nMastering aggregation functions will significantly enhance your data analysis capabilities! 🎯📈"
  },
  {
    "objectID": "posts/pandas/Data Cleaning.html",
    "href": "posts/pandas/Data Cleaning.html",
    "title": "Data Cleaning with Pandas",
    "section": "",
    "text": "Welcome to this tutorial on data cleaning using Pandas! Data cleaning is a crucial step in any data analysis workflow. In this notebook, we’ll cover two essential techniques: - Handling duplicates: Removing or managing repeated rows. - Detecting and removing outliers: Using statistical methods like IQR (Interquartile Range).\nBy the end, you’ll have practical skills to preprocess messy datasets effectively."
  },
  {
    "objectID": "posts/pandas/Data Cleaning.html#setting-up-and-creating-sample-data",
    "href": "posts/pandas/Data Cleaning.html#setting-up-and-creating-sample-data",
    "title": "Data Cleaning with Pandas",
    "section": "1. Setting Up and Creating Sample Data",
    "text": "1. Setting Up and Creating Sample Data\nFirst, let’s import Pandas and create a sample DataFrame to work with.\n\nimport pandas as pd\n\ndata1 = {\n    'A': [1,2,2,3,3],\n    'B': [4,5,5,6,7]\n}\n\ndf1 = pd.DataFrame(data1)\ndf1\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n2\n2\n5\n\n\n3\n3\n6\n\n\n4\n3\n7"
  },
  {
    "objectID": "posts/pandas/Data Cleaning.html#dealing-with-duplicates",
    "href": "posts/pandas/Data Cleaning.html#dealing-with-duplicates",
    "title": "Data Cleaning with Pandas",
    "section": "2. Dealing with Duplicates",
    "text": "2. Dealing with Duplicates\nDuplicates can skew your analysis. Pandas provides easy methods to detect and remove them.\n\nChecking for Duplicates\n\ndf1.duplicated().sum()\n\nnp.int64(1)\n\n\n\ndf1.drop_duplicates()\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n3\n3\n6\n\n\n4\n3\n7\n\n\n\n\n\n\n\n\ndf1.drop_duplicates(subset=['A'])\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n4\n\n\n1\n2\n5\n\n\n3\n3\n6"
  },
  {
    "objectID": "posts/pandas/Data Cleaning.html#handling-outliers",
    "href": "posts/pandas/Data Cleaning.html#handling-outliers",
    "title": "Data Cleaning with Pandas",
    "section": "3. Handling Outliers",
    "text": "3. Handling Outliers\nOutliers are extreme values that can distort statistical analysis. We’ll use the IQR method to detect and filter them.\n\nCreating Sample Data with Outliers\n\n\ndata2 = {\n    'A': [1,2,2,3,11, 11],\n    'B': [1,5,5,6,12, 25]\n}\n\ndf2 = pd.DataFrame(data2)\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1\n\n\n1\n2\n5\n\n\n2\n2\n5\n\n\n3\n3\n6\n\n\n4\n11\n12\n\n\n5\n11\n25\n\n\n\n\n\n\n\n\ndf2.describe()\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\ncount\n6.000000\n6.000000\n\n\nmean\n5.000000\n9.000000\n\n\nstd\n4.690416\n8.602325\n\n\nmin\n1.000000\n1.000000\n\n\n25%\n2.000000\n5.000000\n\n\n50%\n2.500000\n5.500000\n\n\n75%\n9.000000\n10.500000\n\n\nmax\n11.000000\n25.000000\n\n\n\n\n\n\n\n\n\nCalculating IQR and Bounds\n\nq_low = df2['B'].quantile(0.25)\nq_high = df2['B'].quantile(0.75)\n\niqr = q_high - q_low\n\n\nl_bound = q_low - 1.5 * iqr\nu_bound = q_high + 1.5 * iqr  # Fixed: upper bound should be plus, not minus\n\nprint(f\"Lower bound: {l_bound}\")\nprint(f\"Upper bound: {u_bound}\")\n\n\ndf2\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1\n\n\n1\n2\n5\n\n\n2\n2\n5\n\n\n3\n3\n6\n\n\n4\n11\n12\n\n\n5\n11\n25\n\n\n\n\n\n\n\n\n\nFiltering Out Outliers\n\ndf_filtered = df2[(df2['B']&gt;l_bound) & (df2['B']&lt;u_bound)]\ndf_filtered\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\n1\n1\n\n\n1\n2\n5\n\n\n2\n2\n5\n\n\n3\n3\n6\n\n\n4\n11\n12"
  },
  {
    "objectID": "posts/pandas/Data Viz with panda tools.html",
    "href": "posts/pandas/Data Viz with panda tools.html",
    "title": "Data Viz with pandas tools",
    "section": "",
    "text": "This notebook shows simple, reusable examples of creating line, bar, and scatter charts directly from pandas using matplotlib as the backend. Each section includes a short explanation and code snippet.\n\n# Import libraries and create sample DataFrame\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndata = {\n    'Year': [2015,2016,2017,2018,2019],\n    'Revenue': [500,700,650,800,950]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nYear\nRevenue\n\n\n\n\n0\n2015\n500\n\n\n1\n2016\n700\n\n\n2\n2017\n650\n\n\n3\n2018\n800\n\n\n4\n2019\n950\n\n\n\n\n\n\n\n\ndf.plot(x='Year', y='Revenue', kind='line', marker='o', color='black', legend=False)\nplt.title('Revenue over years')\nplt.xlabel('Year')\nplt.ylabel('Revenue')\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nLine charts are useful for showing trends over time. Use pandas’ DataFrame.plot(kind='line') for quick exploration.\n\n# Bar chart example data\ndata = {\n    'City': ['New York', 'London', 'Tokyo', 'Sydney', 'Paris'],\n    'Population': [850000, 890000, 900000, 520000, 1100000]\n}\n\ndf_cities = pd.DataFrame(data)\ndf_cities\n\n\n\n\n\n\n\n\nCity\nPopulation\n\n\n\n\n0\nNew York\n850000\n\n\n1\nLondon\n890000\n\n\n2\nTokyo\n900000\n\n\n3\nSydney\n520000\n\n\n4\nParis\n1100000\n\n\n\n\n\n\n\n\n# Bar chart (sorted by population)\ndf_cities.sort_values(by='Population', ascending=False).plot(x='City', y='Population', kind='bar', color='tab:red', legend=False)\nplt.title('Population in Major Cities')\nplt.xlabel('City')\nplt.ylabel('Population')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nScatter plots are useful to visualize relationships between two numeric variables. Use plt.scatter or DataFrame.plot(kind='scatter').\n\n# Scatter data\ndf_scatter = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 3, 4, 8, 10]})\ndf_scatter\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n3\n4\n8\n\n\n4\n5\n10\n\n\n\n\n\n\n\n\n# Scatter plot\nplt.scatter(df_scatter['X'], df_scatter['Y'], color='g')\nplt.title('Scatter: X vs Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\nUse descriptive titles and axis labels.\nPrefer plt.tight_layout() after plotting to avoid overlap in labels.\nFor large datasets, consider sampling or using alpha blending (alpha) to avoid over-plotting.\nUse color palettes from matplotlib or seaborn for consistent visuals.\n\n\n\n\nThis notebook provided quick examples of line, bar, and scatter charts using pandas + matplotlib. Use these snippets as a starting point for exploratory data analysis and adapt styling as needed."
  },
  {
    "objectID": "posts/pandas/Data Viz with panda tools.html#line-chart-revenue-over-years",
    "href": "posts/pandas/Data Viz with panda tools.html#line-chart-revenue-over-years",
    "title": "Data Viz with pandas tools",
    "section": "",
    "text": "Line charts are useful for showing trends over time. Use pandas’ DataFrame.plot(kind='line') for quick exploration.\n\n# Bar chart example data\ndata = {\n    'City': ['New York', 'London', 'Tokyo', 'Sydney', 'Paris'],\n    'Population': [850000, 890000, 900000, 520000, 1100000]\n}\n\ndf_cities = pd.DataFrame(data)\ndf_cities\n\n\n\n\n\n\n\n\nCity\nPopulation\n\n\n\n\n0\nNew York\n850000\n\n\n1\nLondon\n890000\n\n\n2\nTokyo\n900000\n\n\n3\nSydney\n520000\n\n\n4\nParis\n1100000\n\n\n\n\n\n\n\n\n# Bar chart (sorted by population)\ndf_cities.sort_values(by='Population', ascending=False).plot(x='City', y='Population', kind='bar', color='tab:red', legend=False)\nplt.title('Population in Major Cities')\nplt.xlabel('City')\nplt.ylabel('Population')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/pandas/Data Viz with panda tools.html#scatter-plot-relationship-between-x-and-y",
    "href": "posts/pandas/Data Viz with panda tools.html#scatter-plot-relationship-between-x-and-y",
    "title": "Data Viz with pandas tools",
    "section": "",
    "text": "Scatter plots are useful to visualize relationships between two numeric variables. Use plt.scatter or DataFrame.plot(kind='scatter').\n\n# Scatter data\ndf_scatter = pd.DataFrame({'X': [1, 2, 3, 4, 5], 'Y': [2, 3, 4, 8, 10]})\ndf_scatter\n\n\n\n\n\n\n\n\nX\nY\n\n\n\n\n0\n1\n2\n\n\n1\n2\n3\n\n\n2\n3\n4\n\n\n3\n4\n8\n\n\n4\n5\n10\n\n\n\n\n\n\n\n\n# Scatter plot\nplt.scatter(df_scatter['X'], df_scatter['Y'], color='g')\nplt.title('Scatter: X vs Y')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.grid(alpha=0.3)\nplt.show()"
  },
  {
    "objectID": "posts/pandas/Data Viz with panda tools.html#best-practices",
    "href": "posts/pandas/Data Viz with panda tools.html#best-practices",
    "title": "Data Viz with pandas tools",
    "section": "",
    "text": "Use descriptive titles and axis labels.\nPrefer plt.tight_layout() after plotting to avoid overlap in labels.\nFor large datasets, consider sampling or using alpha blending (alpha) to avoid over-plotting.\nUse color palettes from matplotlib or seaborn for consistent visuals."
  },
  {
    "objectID": "posts/pandas/Data Viz with panda tools.html#summary",
    "href": "posts/pandas/Data Viz with panda tools.html#summary",
    "title": "Data Viz with pandas tools",
    "section": "",
    "text": "This notebook provided quick examples of line, bar, and scatter charts using pandas + matplotlib. Use these snippets as a starting point for exploratory data analysis and adapt styling as needed."
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html",
    "href": "posts/pandas/Handling Categorical Datas.html",
    "title": "Handling Categorical Data with Pandas",
    "section": "",
    "text": "This notebook covers essential techniques for working with categorical data in Pandas, including: - Encoding Methods: Converting categorical variables to numerical formats - Grouping Operations: Analyzing category distributions and aggregations - Data Transformation: Reshaping data with melt and pivot operations\nCategorical data transformation is crucial for machine learning models that require numerical inputs."
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html#setting-up-sample-data",
    "href": "posts/pandas/Handling Categorical Datas.html#setting-up-sample-data",
    "title": "Handling Categorical Data with Pandas",
    "section": "1. Setting Up Sample Data",
    "text": "1. Setting Up Sample Data\nLet’s start by creating a sample DataFrame with categorical data to work with.\n\nimport pandas as pd\n\ndata = {\n    'Category': ['A','B','C','C','B','A']\n}\n\ndf = pd.DataFrame(data)\n\n\ndf\n\n\n\n\n\n\n\n\nCategory\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nC\n\n\n4\nB\n\n\n5\nA"
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html#encoding-categorical-data",
    "href": "posts/pandas/Handling Categorical Datas.html#encoding-categorical-data",
    "title": "Handling Categorical Data with Pandas",
    "section": "2. Encoding Categorical Data",
    "text": "2. Encoding Categorical Data\nMachine learning algorithms typically require numerical inputs. Categorical encoding converts text categories into numbers. Here are the most common techniques:\n\nOne-Hot Encoding\nOne-hot encoding creates binary columns for each category. It’s ideal for nominal (unordered) categories.\nPros: No ordinal assumptions, works well with most algorithms Cons: Can create many columns (curse of dimensionality)\n\npd.get_dummies(df['Category'])[['A','B']]\n\n\n\n\n\n\n\n\nA\nB\n\n\n\n\n0\nTrue\nFalse\n\n\n1\nFalse\nTrue\n\n\n2\nFalse\nFalse\n\n\n3\nFalse\nFalse\n\n\n4\nFalse\nTrue\n\n\n5\nTrue\nFalse\n\n\n\n\n\n\n\n\n\nLabel Encoding\nLabel encoding assigns integer values to categories. Use this when categories have a natural order (ordinal data).\nPros: Memory efficient, preserves single column Cons: Implies ordinal relationship even when none exists\n\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\ndf['Category_LabenEncoded'] = label_encoder.fit_transform(df['Category'])\n\ndf\n\n\n\n\n\n\n\n\nCategory\nCategory_LabenEncoded\n\n\n\n\n0\nA\n0\n\n\n1\nB\n1\n\n\n2\nC\n2\n\n\n3\nC\n2\n\n\n4\nB\n1\n\n\n5\nA\n0\n\n\n\n\n\n\n\n\nimport pandas as pd\n\ndata = {\n    'Category': ['A','B','C','C','B','A']\n}\n\ndf = pd.DataFrame(data)\n\ndf\n\n\n\n\n\n\n\n\nCategory\n\n\n\n\n0\nA\n\n\n1\nB\n\n\n2\nC\n\n\n3\nC\n\n\n4\nB\n\n\n5\nA"
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html#analyzing-categorical-data-with-grouping",
    "href": "posts/pandas/Handling Categorical Datas.html#analyzing-categorical-data-with-grouping",
    "title": "Handling Categorical Data with Pandas",
    "section": "3. Analyzing Categorical Data with Grouping",
    "text": "3. Analyzing Categorical Data with Grouping\nGrouping operations help you understand the distribution and patterns in your categorical data. This is essential for exploratory data analysis.\n\nCounting Category Frequencies\nUse groupby().size() or groupby().count() to see how many times each category appears.\n\ndf.groupby('Category').size()\n\nCategory\nA    2\nB    2\nC    2\ndtype: int64\n\n\n\ndf.groupby('Category').agg({'Category':'count'})\n\n\n\n\n\n\n\n\nCategory\n\n\nCategory\n\n\n\n\n\nA\n2\n\n\nB\n2\n\n\nC\n2"
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html#data-transformation-reshaping-with-melt-and-pivot",
    "href": "posts/pandas/Handling Categorical Datas.html#data-transformation-reshaping-with-melt-and-pivot",
    "title": "Handling Categorical Data with Pandas",
    "section": "4. Data Transformation: Reshaping with Melt and Pivot",
    "text": "4. Data Transformation: Reshaping with Melt and Pivot\nData reshaping is crucial for transforming your data between “wide” and “long” formats. This is particularly useful when working with categorical data across multiple variables.\n\nWide to Long Format (melt)\npd.melt() unpivots a DataFrame from wide format to long format. This is useful for: - Converting multiple categorical columns into a single column - Preparing data for visualization libraries - Making data more database-friendly\n\n# Reshaping Data\ndata = {\n    'Name': ['John', 'Emily', 'Kate'],\n    'Math': [90, 85,88],\n    'Science': [92, 80, 95]\n}\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nName\nMath\nScience\n\n\n\n\n0\nJohn\n90\n92\n\n\n1\nEmily\n85\n80\n\n\n2\nKate\n88\n95\n\n\n\n\n\n\n\n\ndf_melted = pd.melt(df, id_vars='Name', var_name='Subject', value_name='Score')\ndf_melted\n\n\n\n\n\n\n\n\nName\nSubject\nScore\n\n\n\n\n0\nJohn\nMath\n90\n\n\n1\nEmily\nMath\n85\n\n\n2\nKate\nMath\n88\n\n\n3\nJohn\nScience\n92\n\n\n4\nEmily\nScience\n80\n\n\n5\nKate\nScience\n95\n\n\n\n\n\n\n\n\n\nLong to Wide Format (pivot)\ndf.pivot() does the opposite of melt - it converts long format back to wide format. This is useful for: - Creating summary tables - Preparing data for certain types of analysis - Making data more human-readable\n\ndf_melted.pivot(index='Name', columns='Subject', values='Score')\n\n\n\n\n\n\n\nSubject\nMath\nScience\n\n\nName\n\n\n\n\n\n\nEmily\n85\n80\n\n\nJohn\n90\n92\n\n\nKate\n88\n95"
  },
  {
    "objectID": "posts/pandas/Handling Categorical Datas.html#summary",
    "href": "posts/pandas/Handling Categorical Datas.html#summary",
    "title": "Handling Categorical Data with Pandas",
    "section": "Summary",
    "text": "Summary\nIn this notebook, you learned essential data transformation techniques for categorical data:\n\nEncoding: Convert text categories to numbers\n\nOne-hot encoding for nominal data\nLabel encoding for ordinal data\n\nGrouping: Analyze category distributions\n\nCount frequencies with groupby().size()\nAggregate data by categories\n\nReshaping: Transform data structure\n\nmelt(): Wide to long format\npivot(): Long to wide format\n\n\nThese techniques form the foundation of data preprocessing for machine learning and analysis workflows. Choose the right method based on your data characteristics and modeling requirements!\nNext Steps: Practice with real datasets and explore advanced encoding techniques like target encoding or frequency encoding."
  },
  {
    "objectID": "posts/pandas/index.html",
    "href": "posts/pandas/index.html",
    "title": "Pandas for Data Science",
    "section": "",
    "text": "DataFrames and Series - Core data structures and creation\nData Loading - Reading CSV, Excel, JSON, and databases\nData Cleaning - Handling missing values and duplicates\nData Selection - Indexing, filtering, and querying data\nData Transformation - Grouping, aggregation, and pivot tables\nTime Series - Working with dates and time-based data\nData Visualization - Basic plotting with Pandas\nPerformance Tips - Optimizing operations for large datasets"
  },
  {
    "objectID": "posts/pandas/index.html#topics-covered",
    "href": "posts/pandas/index.html#topics-covered",
    "title": "Pandas for Data Science",
    "section": "",
    "text": "DataFrames and Series - Core data structures and creation\nData Loading - Reading CSV, Excel, JSON, and databases\nData Cleaning - Handling missing values and duplicates\nData Selection - Indexing, filtering, and querying data\nData Transformation - Grouping, aggregation, and pivot tables\nTime Series - Working with dates and time-based data\nData Visualization - Basic plotting with Pandas\nPerformance Tips - Optimizing operations for large datasets"
  },
  {
    "objectID": "posts/pandas/index.html#learning-path",
    "href": "posts/pandas/index.html#learning-path",
    "title": "Pandas for Data Science",
    "section": "Learning Path",
    "text": "Learning Path\n\nStart with basic DataFrame operations\nLearn data loading and inspection techniques\nMaster data cleaning and preprocessing\nExplore advanced transformations and analysis\nPractice with real-world datasets"
  },
  {
    "objectID": "posts/pandas/index.html#available-notebooks",
    "href": "posts/pandas/index.html#available-notebooks",
    "title": "Pandas for Data Science",
    "section": "Available Notebooks",
    "text": "Available Notebooks"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html",
    "href": "posts/pandas/learn pandas Df.html",
    "title": "Pandas: Intro to DataFrames",
    "section": "",
    "text": "DataFrames are the core data structure in pandas for tabular data. This notebook covers creating DataFrames from various sources and basic operations.\nYou will learn how to: - Import essential libraries (Pandas, Seaborn, Matplotlib, NumPy) - Create DataFrames from dictionaries, lists, and NumPy arrays - View and manipulate DataFrames - Export DataFrames to CSV and Excel files"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#importing-libraries",
    "href": "posts/pandas/learn pandas Df.html#importing-libraries",
    "title": "Pandas: Intro to DataFrames",
    "section": "Importing Libraries",
    "text": "Importing Libraries\nStart by importing pandas and other useful libraries for data analysis and visualization.\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#creating-dataframes-from-dictionaries",
    "href": "posts/pandas/learn pandas Df.html#creating-dataframes-from-dictionaries",
    "title": "Pandas: Intro to DataFrames",
    "section": "Creating DataFrames from Dictionaries",
    "text": "Creating DataFrames from Dictionaries\nDataFrames can be created from Python dictionaries where keys become column names.\n\ndata = {'Name': ['Adil', 'Aman', 'Ziya', 'Zahra'],\n        'Age': [23,19,15,9],\n        'City': ['Matannur','Vellore', 'Tly', 'Knr' ]\n        }\ndata\n\n{'Name': ['Adil', 'Aman', 'Ziya', 'Zahra'],\n 'Age': [23, 19, 15, 9],\n 'City': ['Matannur', 'Vellore', 'Tly', 'Knr']}\n\n\n\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#creating-dataframes-from-lists",
    "href": "posts/pandas/learn pandas Df.html#creating-dataframes-from-lists",
    "title": "Pandas: Intro to DataFrames",
    "section": "Creating DataFrames from Lists",
    "text": "Creating DataFrames from Lists\nYou can also create DataFrames from lists of lists, specifying column names.\n\ndata_list = [ \n    ['Adil', 23, 'Mattanur'],\n    ['Aman', 19, 'Vellore'],\n    ['Siraj', 55, 'Tly'],\n    ['Faritha', 40, 'Chokli']\n    ]\n\ndata_list\n\n[['Adil', 23, 'Mattanur'],\n ['Aman', 19, 'Vellore'],\n ['Siraj', 55, 'Tly'],\n ['Faritha', 40, 'Chokli']]\n\n\n\ndf_list = pd.DataFrame(data_list, columns=['Name', 'Age', 'City'])\ndf_list\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMattanur\n\n\n1\nAman\n19\nVellore\n\n\n2\nSiraj\n55\nTly\n\n\n3\nFaritha\n40\nChokli"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#creating-dataframes-from-numpy-arrays",
    "href": "posts/pandas/learn pandas Df.html#creating-dataframes-from-numpy-arrays",
    "title": "Pandas: Intro to DataFrames",
    "section": "Creating DataFrames from NumPy Arrays",
    "text": "Creating DataFrames from NumPy Arrays\nPandas integrates with NumPy; create DataFrames from arrays with column names.\n\nimport numpy as np\n\n\ndata_array = np.array([[1,2,3],\n                       [4,5,6],\n                       [7,8,9]])\n\ndata_array\n\narray([[1, 2, 3],\n       [4, 5, 6],\n       [7, 8, 9]])\n\n\n\ndf_array = pd.DataFrame(data_array, columns=['A', 'B', 'C'])\ndf_array\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n2\n3\n\n\n1\n4\n5\n6\n\n\n2\n7\n8\n9"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#exporting-dataframes",
    "href": "posts/pandas/learn pandas Df.html#exporting-dataframes",
    "title": "Pandas: Intro to DataFrames",
    "section": "Exporting DataFrames",
    "text": "Exporting DataFrames\nSave DataFrames to files like CSV or Excel for sharing or further analysis.\n\ndf.to_csv('example.csv', index=False)\n\n\ndf.to_excel('example.xlsx', index=False)"
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#best-practices",
    "href": "posts/pandas/learn pandas Df.html#best-practices",
    "title": "Pandas: Intro to DataFrames",
    "section": "Best Practices",
    "text": "Best Practices\n\nUse descriptive column names.\nCheck data types with df.dtypes after creation.\nHandle missing data appropriately."
  },
  {
    "objectID": "posts/pandas/learn pandas Df.html#summary",
    "href": "posts/pandas/learn pandas Df.html#summary",
    "title": "Pandas: Intro to DataFrames",
    "section": "Summary",
    "text": "Summary\nThis notebook introduced creating and exporting DataFrames. DataFrames are versatile for data manipulation—explore more operations next!"
  },
  {
    "objectID": "posts/pandas/loadsata.html",
    "href": "posts/pandas/loadsata.html",
    "title": "Pandas: Data Loading",
    "section": "",
    "text": "Pandas is a powerful Python library for data analysis and manipulation. It provides easy-to-use data structures and functions for working with structured data.\nIn this notebook, you will learn how to:\n\nImport the Pandas library\nLoad data from a CSV file\nLoad data from an Excel file\nView the loaded data\n\n\nimport pandas as pd\n\n\ndf_csv = pd.read_csv('example.csv')\ndf_csv\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr\n\n\n\n\n\n\n\n\ndf_xl = pd.read_excel('example.xlsx')\ndf_xl\n\n\n\n\n\n\n\n\nName\nAge\nCity\n\n\n\n\n0\nAdil\n23\nMatannur\n\n\n1\nAman\n19\nVellore\n\n\n2\nZiya\n15\nTly\n\n\n3\nZahra\n9\nKnr"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html",
    "title": "Pandas: Dealing with missing Datas",
    "section": "",
    "text": "Import Pandas and NumPy, which are essential for data manipulation and handling missing values.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#import-libraries",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#import-libraries",
    "title": "Pandas: Dealing with missing Datas",
    "section": "",
    "text": "Import Pandas and NumPy, which are essential for data manipulation and handling missing values.\n\nimport pandas as pd\nimport numpy as np"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#what-is-np.nan",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#what-is-np.nan",
    "title": "Pandas: Dealing with missing Datas",
    "section": "What is np.nan?",
    "text": "What is np.nan?\nnp.nan represents a missing value (“Not a Number”) in NumPy and Pandas.\n\nnp.nan\n\nnan"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-with-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-with-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Create DataFrame with Missing Values",
    "text": "Create DataFrame with Missing Values\nThis section creates a DataFrame containing missing values using np.nan.\n\ndata = {'A': [1,2,np.nan,4,5],\n        'B': [6,np.nan,7,8,9],\n        'C': [11,12,13,np.nan,15]\n        }\ndf = pd.DataFrame(data)\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n6.0\n11.0\n\n\n1\n2.0\nNaN\n12.0\n\n\n2\nNaN\n7.0\n13.0\n\n\n3\n4.0\n8.0\nNaN\n\n\n4\n5.0\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#detect-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#detect-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Detect Missing Values",
    "text": "Detect Missing Values\nUse isnull() to check which values are missing in the DataFrame.\n\ndf.isnull()\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\nFalse\nFalse\nFalse\n\n\n1\nFalse\nTrue\nFalse\n\n\n2\nTrue\nFalse\nFalse\n\n\n3\nFalse\nFalse\nTrue\n\n\n4\nFalse\nFalse\nFalse"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#count-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#count-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Count Missing Values",
    "text": "Count Missing Values\nUse isnull().sum() to count the number of missing values in each column.\n\ndf.isnull().sum()\n\nA    1\nB    1\nC    1\ndtype: int64"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-rows-with-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-rows-with-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Drop Rows with Missing Values",
    "text": "Drop Rows with Missing Values\nUse dropna() to remove rows containing missing values from the DataFrame.\n\n# Drops rows with na values\ndf.dropna(inplace=True)\n# or\ndf = df.dropna()"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#view-dataframe-after-dropping-rows",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#view-dataframe-after-dropping-rows",
    "title": "Pandas: Dealing with missing Datas",
    "section": "View DataFrame After Dropping Rows",
    "text": "View DataFrame After Dropping Rows\nDisplay the DataFrame after removing rows with missing values.\n\ndf\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n6.0\n11.0\n\n\n4\n5.0\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#reset-index-after-dropping-rows",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#reset-index-after-dropping-rows",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Reset Index After Dropping Rows",
    "text": "Reset Index After Dropping Rows\nUse reset_index(drop=True) to reset the DataFrame index after dropping rows.\n\ndf.reset_index(drop=True)\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1.0\n6.0\n11.0\n\n\n1\n5.0\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#create-another-dataframe-with-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#create-another-dataframe-with-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Create Another DataFrame with Missing Values",
    "text": "Create Another DataFrame with Missing Values\nThis section creates a new DataFrame with missing values for further operations.\n\ndata1 = {'A': [1,2,3,4,5],\n        'B': [6,np.nan,7,8,9],\n        'C': [11,12,13,np.nan,15]\n        }\ndf1 = pd.DataFrame(data1)\ndf1\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\nNaN\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n3\n4\n8.0\nNaN\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-columns-with-missing-values",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-columns-with-missing-values",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Drop Columns with Missing Values",
    "text": "Drop Columns with Missing Values\nUse dropna(axis=1) to remove columns containing missing values from the DataFrame.\n\ndf1 = df1.dropna(axis=1)\ndf1\n\n\n\n\n\n\n\n\nA\n\n\n\n\n0\n1\n\n\n1\n2\n\n\n2\n3\n\n\n3\n4\n\n\n4\n5"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-for-threshold-example",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-for-threshold-example",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Create DataFrame for Threshold Example",
    "text": "Create DataFrame for Threshold Example\nThis section creates a DataFrame to demonstrate dropping rows based on a threshold of non-missing values.\n\ndata2 = {'A': [1,2,3,4,5],\n        'B': [6,np.nan,7,np.nan,9],\n        'C': [11,12,13,np.nan,15]\n        }\ndf2 = pd.DataFrame(data2)\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\nNaN\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n3\n4\nNaN\nNaN\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-rows-based-on-threshold",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#drop-rows-based-on-threshold",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Drop Rows Based on Threshold",
    "text": "Drop Rows Based on Threshold\nUse dropna(thresh=2) to keep only rows with at least 2 non-missing values.\n\ndf2 = df2.dropna(thresh=2)\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\nNaN\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-zero",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-zero",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Fill Missing Values with Zero",
    "text": "Fill Missing Values with Zero\nUse fillna(0) to replace all missing values in the DataFrame with zero.\n\ndf2 = df2.fillna(0)\ndf2\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\n0.0\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-for-fill-methods",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#create-dataframe-for-fill-methods",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Create DataFrame for Fill Methods",
    "text": "Create DataFrame for Fill Methods\nThis section creates a DataFrame to demonstrate different methods for filling missing values.\n\ndata3 = {'A': [1,2,3,4,5],\n        'B': [6,np.nan,7,np.nan,9],\n        'C': [11,12,13,np.nan,15]\n        }\ndf3 = pd.DataFrame(data2)\ndf3\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\nNaN\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n3\n4\nNaN\nNaN\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-mean-or-median",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-mean-or-median",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Fill Missing Values with Mean or Median",
    "text": "Fill Missing Values with Mean or Median\nUse fillna(df.mean()) or fillna(df.median()) to replace missing values with the mean or median of each column.\n\ndf3.fillna(df3.mean())\ndf3.fillna(df3.median())\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\n7.0\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n3\n4\n7.0\n12.5\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-forwardbackward-fill",
    "href": "posts/pandas/Pandas-Dealing with missing Datas.html#fill-missing-values-with-forwardbackward-fill",
    "title": "Pandas: Dealing with missing Datas",
    "section": "Fill Missing Values with Forward/Backward Fill",
    "text": "Fill Missing Values with Forward/Backward Fill\nUse fillna(method='ffill') for forward fill and fillna(method='bfill') for backward fill to propagate non-missing values.\n\ndf3.fillna(method='ffill')\ndf3.fillna(method='bfill')\n\nC:\\Users\\adila\\AppData\\Local\\Temp\\ipykernel_7712\\3709391602.py:1: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df3.fillna(method='ffill')\nC:\\Users\\adila\\AppData\\Local\\Temp\\ipykernel_7712\\3709391602.py:2: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n  df3.fillna(method='bfill')\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n6.0\n11.0\n\n\n1\n2\n7.0\n12.0\n\n\n2\n3\n7.0\n13.0\n\n\n3\n4\n9.0\n15.0\n\n\n4\n5\n9.0\n15.0"
  },
  {
    "objectID": "posts/pandas/Time Series Visualisation.html",
    "href": "posts/pandas/Time Series Visualisation.html",
    "title": "Time Series Visualization with Pandas",
    "section": "",
    "text": "Time series data visualization is crucial for understanding trends, patterns, and changes over time. This notebook covers:\nEffective visualization helps you communicate temporal patterns and insights from your data."
  },
  {
    "objectID": "posts/pandas/Time Series Visualisation.html#creating-time-series-data",
    "href": "posts/pandas/Time Series Visualisation.html#creating-time-series-data",
    "title": "Time Series Visualization with Pandas",
    "section": "1. Creating Time Series Data",
    "text": "1. Creating Time Series Data\nLet’s start by creating time series data using Pandas’ powerful datetime capabilities.\n\nCreating a Time Series with DateTime Index\nUse pd.date_range() to create a sequence of dates, then create a Series with datetime index:\n\nimport pandas as pd\n\ntime_index = pd.date_range('2025-01-01', periods=5, freq='D')\nts_data = pd.Series([100,120,80,110,90], index=time_index)\n\nts_data\n\n2025-01-01    100\n2025-01-02    120\n2025-01-03     80\n2025-01-04    110\n2025-01-05     90\nFreq: D, dtype: int64\n\n\n\n\nConverting Series to DataFrame\nYou can easily convert a time series Series to a DataFrame for additional operations:\n\ndf = pd.DataFrame(ts_data)\ndf\n\n\n\n\n\n\n\n\n0\n\n\n\n\n2025-01-01\n100\n\n\n2025-01-02\n120\n\n\n2025-01-03\n80\n\n\n2025-01-04\n110\n\n\n2025-01-05\n90"
  },
  {
    "objectID": "posts/pandas/Time Series Visualisation.html#basic-time-series-visualization",
    "href": "posts/pandas/Time Series Visualisation.html#basic-time-series-visualization",
    "title": "Time Series Visualization with Pandas",
    "section": "2. Basic Time Series Visualization",
    "text": "2. Basic Time Series Visualization\nPandas integrates seamlessly with Matplotlib for creating time series plots. The .plot() method provides an easy interface for visualization.\n\nLine Plot with Customization\nCreate a line plot with markers, custom colors, and styling:\n\nimport matplotlib.pyplot as plt\n\nts_data.plot(kind='line', marker='o', color='b', linestyle='--')\nplt.xlabel('Date')\nplt.ylabel('Value')\n\nplt.grid()\nplt.show()"
  },
  {
    "objectID": "posts/pandas/Time Series Visualisation.html#summary",
    "href": "posts/pandas/Time Series Visualisation.html#summary",
    "title": "Time Series Visualization with Pandas",
    "section": "Summary",
    "text": "Summary\nTime series visualization is essential for understanding temporal patterns in your data. In this notebook, you learned:\n\n📅 Time Series Creation\n\npd.date_range(): Create sequences of dates\nDatetime indexing: Set dates as DataFrame/Series index\nSeries to DataFrame conversion\n\n\n\n📊 Visualization Techniques\n\nLine plots: Basic time series visualization with .plot()\nCustomization: Colors, markers, line styles, and labels\nMatplotlib integration: Pandas plotting with Matplotlib backend\n\n\n\n🎨 Plot Customization Options\n\nkind='line': Line plot type\nmarker='o': Data point markers\ncolor='b': Line colors\nlinestyle='--': Line styles (solid, dashed, etc.)\nplt.xlabel(), plt.ylabel(): Axis labels\nplt.grid(): Add grid lines\n\n\n\n💡 Key Concepts\n\nDatetime Index: Essential for time series operations\nPlot Method: Pandas’ built-in plotting interface\nMatplotlib Integration: Customize plots with full Matplotlib control\nClear Labeling: Always label axes and add titles\n\n\n\n🚀 Best Practices\n\nUse appropriate date ranges for your analysis period\nChoose colors and styles that enhance readability\nAdd grid lines for better value estimation\nLabel axes clearly for context\n\n\n\n📈 Next Steps\n\nExplore advanced plots (area plots, bar plots)\nLearn about subplots for multiple time series\nPractice with real time series datasets\nExperiment with different styling options\n\nMastering time series visualization will help you effectively communicate temporal trends and patterns! 📊⏰"
  },
  {
    "objectID": "posts/python/index.html",
    "href": "posts/python/index.html",
    "title": "Python Fundamentals",
    "section": "",
    "text": "Modules and Packages - Code organization and imports\nFunctions and Classes - Object-oriented programming\nData Structures - Lists, dictionaries, sets, tuples\nFile Handling - Reading and writing data\nError Handling - Try/except and debugging\nLibraries - NumPy, Pandas, Matplotlib basics"
  },
  {
    "objectID": "posts/python/index.html#topics-covered",
    "href": "posts/python/index.html#topics-covered",
    "title": "Python Fundamentals",
    "section": "",
    "text": "Modules and Packages - Code organization and imports\nFunctions and Classes - Object-oriented programming\nData Structures - Lists, dictionaries, sets, tuples\nFile Handling - Reading and writing data\nError Handling - Try/except and debugging\nLibraries - NumPy, Pandas, Matplotlib basics"
  },
  {
    "objectID": "posts/python/index.html#learning-path",
    "href": "posts/python/index.html#learning-path",
    "title": "Python Fundamentals",
    "section": "Learning Path",
    "text": "Learning Path\nStart with modules and packages, then progress through each topic systematically."
  },
  {
    "objectID": "posts/python/index.html#available-notebooks",
    "href": "posts/python/index.html#available-notebooks",
    "title": "Python Fundamentals",
    "section": "Available Notebooks",
    "text": "Available Notebooks"
  }
]